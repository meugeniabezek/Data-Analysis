{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin: 0; text-align:right;\">2022-2 · Master universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informatica, Multimedia y Telecomunicaciones</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "### Modelos generativos\n",
    "\n",
    "En esta práctica implementaremos uno de los tipos de modelos generativos más utilizados actualmente, las redes generativas adversarias, ie. **GANs**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El objetivo de esta PEC es comprender la implementación de una solución generativa, utilizando DCGANs para la generación de imágenes, mediante el conjunto de datos de referencia en deep learning más sencillo existente: MNIST.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:17:55.591356Z",
     "iopub.status.busy": "2023-06-08T12:17:55.590771Z",
     "iopub.status.idle": "2023-06-08T12:17:55.599347Z",
     "shell.execute_reply": "2023-06-08T12:17:55.597933Z",
     "shell.execute_reply.started": "2023-06-08T12:17:55.591314Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Obtención de los datos\n",
    "\n",
    "El código para cargar los datos es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:17:58.615581Z",
     "iopub.status.busy": "2023-06-08T12:17:58.615142Z",
     "iopub.status.idle": "2023-06-08T12:17:59.047939Z",
     "shell.execute_reply": "2023-06-08T12:17:59.046819Z",
     "shell.execute_reply.started": "2023-06-08T12:17:58.615552Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "img_channels = 1\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "Añade un comentario explicativo, a cada una de las líneas de código de abajo, indicando cuál es su funcionalidad.</div>\n",
    "\n",
    "**Respuesta**:\n",
    "\n",
    "* `latent_dim = 100`: Es la dimensión del espacio latente, el cual es de menor dimensión que los datos originales y su objetivo es capturar los patrones o estructura subyacentes en los datos para representarlos de forma comprimida y abstracta. En el caso de las GANs, el generador sabe cómo convertir un punto del espacio latente en una imagen (preferiblemente similar al conjunto de datos en el que se entrenó). En este caso, la dimensión latente del modelo generativo o el tamaño de entrada del generador se establece en 100. Entonces, la red del generador toma un vector de entrada aleatorio de tamaño 100 del espacio latente y lo transforma en una muestra de salida\n",
    "\n",
    "* `img_rows, img_cols = 28, 28`: Con esta linea se define el tamaño de las imágenes tanto en ancho (img_cols) como en alto (img_rows)\n",
    "* `img_channels = 1`: Con esta linea se define la cantidad de canales que representarán los colores de las imágenes, en este caso se indica 1, por lo tanto las imagenes serán en escala de grises.\n",
    "\n",
    "* `(x_train, _), (_, _) = mnist.load_data()`: Esta linea carga el conjunto de datos MNIST utilizando la función mnist.load_data(). El conjunto de datos MNIST consiste de un conjunto de imágenes de dígitos escritos a mano. Devuelve cuatro conjuntos de datos: imágenes de entrenamiento (x_train), etiquetas de entrenamiento (que no se usan y se descartan con _), imágenes de prueba (que no se usan y se descartan con _) y etiquetas de prueba (que no se usan y se descartan con _). _)\n",
    "\n",
    "* `x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)`: Esta linea remodela el conjunto de datos x_train para que coincida con la forma de entrada deseada para la red neuronal. El x_train.shape[0] representa el número de muestras de entrenamiento. Remodela el conjunto de datos para que tenga dimensiones [número de muestras, img_rows, img_cols, img_channels].\n",
    "\n",
    "* `x_train = x_train.astype('float32')`: Esta linea convierte el tipo de datos del conjunto x_train del tipo de entero predeterminado a un tipo de punto flotante de 32 bits.\n",
    "\n",
    "* `x_train /= 255`: Esta linea normaliza los valores de píxel del conjunto de datos x_train. Dado que los valores de píxeles en las imágenes varían de 0 a 255 (escala de grises de 8 bits), dividirlos por 255 escala los valores entre 0 y 1. La normalización ayuda a lograr una mejor convergencia durante el entrenamiento y garantiza que todas las entidades de entrada tengan una escala similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementación del Generador\n",
    "\n",
    "A continuación se muestra una propuesta de generador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:01.076567Z",
     "iopub.status.busy": "2023-06-08T12:18:01.076204Z",
     "iopub.status.idle": "2023-06-08T12:18:01.086584Z",
     "shell.execute_reply": "2023-06-08T12:18:01.085627Z",
     "shell.execute_reply.started": "2023-06-08T12:18:01.076538Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_model(): \n",
    "    dropout = 0.4\n",
    "    depth = 256 # 64+64+64+64\n",
    "    dim = 7\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 100\n",
    "    # Out: dim x dim x depth\n",
    "    model.add(Dense(dim*dim*depth, input_dim=latent_dim))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((dim, dim, depth)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # In: dim x dim x depth\n",
    "    # Out: 2*dim x 2*dim x depth/2\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "    model.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "Contesta a las preguntas siguientes:\n",
    "</div>\n",
    "\n",
    "**1. ¿Cuál es la finalidad del generador?:**\n",
    "\n",
    "Su finalidad es generar nuevas instancias del mismo dominio que el conjunto de datos de origen, es decir generar muestras realistas y de alta calidad que se parezcan a los datos de entrenamiento. En otras palabras, el objetivo del generador es generar datos sintéticos que no se pueden distinguir de los datos reales de forma de que el discriminador no pueda diferenciarlos.\n",
    "\n",
    "**2. Investigar por qué se utiliza `Upsampling` en las dos primeras capas en lugar de la `Conv2DTranspose` propuesta en DCGAN. Dar una justificación:**\n",
    "\n",
    "El uso de la capa Upsampling antes de la capa Conv2DTranspose es una técnica utilizada en modelos generativos para generar imágenes realistas y de mayor calidad. El objetivo de la capa de Upsampling es aumentar la resolución espacial de los datos y mejorar la calidad de la salida generada. El generador toma ruido aleatorio o variables latentes como entrada y su objetivo es generar muestras realistas que se asemejen a los datos de entrenamiento. La capa Conv2DTranspose es responsable de aumentar el muestreo de la entrada y transformarla en una salida de mayor resolución. Pero, en algunos casos, el uso de Conv2DTranspose sola puede generar ciertos artefactos o pérdida de detalles finos en las imágenes generadas. Para solucionar esto es que se añade una capa Upsampling antes de la capa Conv2DTranspose. La capa Upsampling aumenta las dimensiones espaciales de los datos mediante distintas técnicas, como interpolación o vecinos más cercanos. Al aumentar el muestreo, la resolución de los mapas de características aumenta lo que permite que el generador capture detalles más finos y genere resultados de mejor calidad.\n",
    "\n",
    "**3. ¿Por qué se utiliza la normalización entre capas?**\n",
    "\n",
    "La capa BatchNormalization tiene como objetivo principal mejorar la estabilidad y el rendimiento de la red neuronal durante el entrenamiento. Opera en un lote de entrada y lo normaliza para tener media cero y varianza unitaria.  Al normalizar las entradas de cada capa, la normalización por lotes ayuda a reducir el problema de desaparición/explosión del gradiente, permitiendo una convergencia más estable y rápida.\n",
    "Otro problema que ayuda a resolver la normalización entre capas es el cambio de covariable interno que ocurre cuando la distribución de las entradas de la capa cambia durante el entrenamiento, lo que dificulta que la red aprenda de manera efectiva, la normalización por lotes soluciona este problema al normalizar las entradas de cada capa, ayudando a que la red sea más resistente a los cambios en la distribución de entrada. \n",
    "Al estabilizar el proceso de entrenamiento interno, la normalización ayuda a mejorar el flujo del gradiente a través de la red, es decir permite una retropropagación más eficiente, lo que a su vez conduce a un mejor aprendizaje y optimización.\n",
    "En el modelo que tenemos aquí, la capa de normalización se aplica justo antes de cada capa de activación, ayudando a mantener las activaciones estables, preservar la información, mejorar la convergencia y proporcionar regularización. Todo esto contribuye a la eficacia y rendimiento del modelo durante el entrenamiento y la generación de muestras de alta calidad.\n",
    "\n",
    "**4. ¿Qué funciones de activación se utilizan? ¿Cuál es la razón de la sigmoide en la última capa?**\n",
    "\n",
    "En la capa de activación pueden usarse distintas funciones de activación dependiendo los requerimientos del modelo y del problema que se intenta resolver, algunos ejemplos son:\n",
    "    \n",
    "    - ReLU (Unidad Lineal Rectificada): Establece todos los valores negativos a cero y deja los positivos sin cambios. Es computacionalmente eficiente y ayuda a mitigar el problema del gradiente. En el modelo del generador que tenemos aquí, se utiliza esta función en las capas intermedias, ReLU apermite que el generador aprenda mapeos complejos desde el espacio latente hasta el mapa de características de mayor dimensión. Lo ayuda a capturar y ampliar las características relevantos en los datos, ayudandolo a generar imágenes más diversas y detalladas.\n",
    "    \n",
    "    - LeakyReLU: Es una variación de la anterior, introduce una variación que es una pequeña pendiente negativa para valores negativos, al permitir valores negativos pequeños ayuda a abordar los puntos donde la función anterior puede  volverse intactiva.\n",
    "   \n",
    "    - Tanh (tangente hiperbolica): Ajusta los valores de entrada entre -1 y 1, proporcionando un rango de valores tanto positivos como negativos. \n",
    "    \n",
    "    - Softmax: Se utiliza a menudo en la capa de salida de problemas de clasificaci'on de múltiples clases. Normaliza las probabilidades de salida para que sumen 1, lo que permite que el modelo asigne probabilidades a varias clases. \n",
    "    \n",
    "    - Lineal: Función lineal o de identidad, simplemente devuelve el valor de entrada sin ninguna transformación, se usa comunmente en tareas de regresión o cuando el modelo necesita aprender relaciones lineales.\n",
    "    \n",
    "    - Sigmoid: Se utiliza particularmente en problemas de clasificación binaria. Asigna a la entrada un rango de [0,1] lo cual lo hace adecuado para tareas en las que la salida representa probabilidades o decisiones binarias. \n",
    "    En el modelo de generador que tenemos aquí, en la última capa de activación se usa una función sigmoid con el objetivo de que la salida del generador tenga valores entre 0 y 1, lo cual representa la intensidad de la escala de grises de cada píxel de la imagen generada. \n",
    "    Es importante el uso de la capa sigmoidea en la última capa para garantizar que las imagenes generadas tengas intensidades de pixeles en el rango adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación del Discriminador\n",
    "\n",
    "A continuación se muestra el discriminador propuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:01.469554Z",
     "iopub.status.busy": "2023-06-08T12:18:01.469189Z",
     "iopub.status.idle": "2023-06-08T12:18:01.479387Z",
     "shell.execute_reply": "2023-06-08T12:18:01.478291Z",
     "shell.execute_reply.started": "2023-06-08T12:18:01.469525Z"
    }
   },
   "outputs": [],
   "source": [
    "# (W−F+2P)/S+1\n",
    "def discriminator_model():\n",
    "    depth = 64\n",
    "    dropout = 0.4\n",
    "    input_shape = (img_rows, img_cols, img_channels)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 28 x 28 x 1, depth = 1\n",
    "    # Out: 14 x 14 x 1, depth=64\n",
    "    model.add(Conv2D(depth, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Out: 1-dim probability\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "Contesta a las preguntas siguientes:\n",
    "</div>\n",
    "\n",
    "\n",
    "**1. ¿Cuál es la finalidad del discriminador?:**\n",
    "\n",
    "La función del discriminador es distinguir entre datos reales y generados. El discriminador actúa como un clasificador binario, entrenado para clasificar las muestras de entrada como reales o falsas. Entonces el generador genera datos sintéticos, como imágenes en este caso, y el discriminador evalúa los datos generados y proporciona retroalimentación al generador, en este sentido podemos mencionar que el discriminador guia el proceso de aprendizaje del generador asegurando que los datos que va generando sean cada vez más realistas.  \n",
    "\n",
    "**2. ¿Cuáles son las dimensiones de los tensores y características de las variables de entrada y salida del discriminador? :**\n",
    "\n",
    "    - Tensor de entrada: La forma del tensor de entrada es (img_rows, img_cols, img_channels), donde img_rows es la altura de la imagen de entrada, img_cols es el ancho de la imagen de entrada e img_channels es el número de canales en la imagen de entrada. En este caso en particular el tamaño de los datos de entrada será: (28,28,1).\n",
    "    \n",
    "    - Capas convolucionales: El discriminador utiliza varias capas convolucionales con diferentes profundidades o número de filtros. La profundidad comienza en 64 y se multiplica por dos para cada capa posterior. Utilizan un kernel de tamaño (5,5) y tienen un stride (paso) de 2, con excepción de la última que tiene stride = 1. El padding se setea en 'same' lo cual significa que las dimensiones espaciales de entrada y salida son iguales.\n",
    "    \n",
    "    - Capas LeakyReLU: Seteadas con un alpha = 0.02, permiten la propagación de pequeños valores negativos para evitar el problema de 'muerte de las neuronas ReLU' que ocurre cuando la función las neuronas ReLU siempre emiten 0 para cualquier entrada y no contribuyen al proceso de aprendizaje, lo cual sucede cuando una gran cantidad de neuronas terminan en el estado saturado, donde la suma ponderada de las entradas es negativa y la función la mapea como cero. Una vez que la neurona \"muere\" es poco probable que se recupere durante el entrenamiento ya que sus gradientes permanecen en cero y no puede actualizar sus pesos.\n",
    "    \n",
    "    - Capas dropout: Se aplica regularización con capas de dropout, esta capa establece aleatoriamente una fracción de las unidades de entrada a 0 durante el entrenamiento para evitar el sobre entrenamiento.\n",
    "    \n",
    "    - Capa de salida: Es una capa totalmente conectada (Dense) con una sola unidad. Produce una puntuación de probabilidad unidimensional que indica la probabilidad de que la imagen de entrada sea real o falsa, la función de activación en esta capa es sigmoide, para asignar a la salida el rango entre 0 y 1. \n",
    "    \n",
    "    - Capa Flatten: Antes de la capa de salida se utiliza una capa flatten para convertir los mapas de características 2D de las capas convolucionales anteriores en vectores de características 1D. \n",
    "    \n",
    "    En conclusión, el discriminador toma una imagen de entrada de forma (img_rows, img_cols, img_channels) y produce una única puntuación que indica la probabilidad de que la entrada sea real o falsa.\n",
    "    \n",
    "    \n",
    "**3. ¿Cuál es la diferencia con una CNN habitual?**\n",
    "\n",
    "Una CNN se utiliza principalmente para tareas de aprendizaje, como la clasificación de imágenes, la detección de objetos y la segmentación. Está diseñada para aprender y extraer características de los datos de entrada para hacer predicciones o clasificaciones. En cambio una GAN es un marco de aprendizaje no supervisado que consta de dos redes neuronales en competencia, un generador y un discriminador, que tienen como objetivo generar datos sintéticos que se asemejen a la distribución de datos reales, como imágenes, audio o texto realistas.\n",
    "Dada esta diferencia entre ambas redes neuronales, la forma de entrenamiento de ellas también varía, las CNN suelen entrenarse de manera supervisada, donde la red se presenta con datos de entrenamiento etiquetados y su objetivo es minimizar una función de pérdida, ajustando los parámetros de la red mediante retropropagación y descenso del gradiente. Las GANs, por otro lado, tienen otra estrategia, el generador y el discriminador se entrenan iterativamente de manera competitiva. El generador tiene como objetivo generar datos que engañen al discriminador, mientras que éste tiene como objetivo clasificar con precisión los datos reales y falsos. Este proceso de entrenamiento implica actualizar los parámetros de las redes en función de los gradientes calculados a partir de la retroalimentación del discriminador.\n",
    "\n",
    "**4. ¿Qué funciones de activación se utilizan?**\n",
    "\n",
    "En el discriminador de aquí las funciones de activación que se utilizan son LeakyReLU y Sigmoid.\n",
    "\n",
    "    - Leaky ReLU: se utiliza después de cada capa convolucional. Como mencioné antes, introduce una pequeña pendiente negativa para entradas negativas evitando que las neuronas \"mueran\" por completo y permitiendo el flujo de gradientes durante la retropropagación.\n",
    "    \n",
    "    - Sigmoid: Se utiliza en la capa final. Ajusta la salida en un rango entre 0 y 1. La salida se interpreta como la probabilidad de que la imagen que recibe el discriminador sea falsa (cercano a 0) o real (cercano a 1).\n",
    "\n",
    "**5. ¿Cuál es la finalidad del dropout que encontramos en las capas?**\n",
    "\n",
    "El objetivo de la capa DropOut es evitar el sobre entrenamiento y mejorar la capacidad de generalización de la red. \n",
    "El sobre entrenamiento se da cuando una red se vuelve muy especializada en los datos de entrenamiento y no logra generalizar bien los datos que no ha visto. La capa de DropOut ayuda a evitar eso eliminando aleatoriamente (es decir ajustando a cero) una fracción de las entradas durante el entrenamiento. \n",
    "\n",
    "En este modelo, la capa DropOut se aplica después de cada capa convolucional y de la activación LeakyReLU. al eliminar entradas de forma aleatoria, el DropOut evita que el discriminador confie demasiado en activaciones o características específicas. Lo cual lleva a la red a aprender representaciones más sólidas y generalizables. La capa DropOut actúa como una especie de regularización, reduciendo las posibilidades de sobre entrenamiento y mejorando la capacidad de discriminador para generalizar datos nuevos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelo GAN\n",
    "\n",
    "\n",
    "\n",
    "**1. A qué llamamos modelo GAN y por qué recibe ese nombre?:**\n",
    "\n",
    "Los modelos GAN reciben ese nombre por Redes Antagónicas Generativas o Redes Adversarias Generativas. Son un método para la optimización competitiva entre dos redes neuronales, una llamada generadora y otra discriminadora, con el objetivo de generar nuevas instancias de datos idealmente indistinguibles a las pertenecientes a la distribución de probabilidad de la que derivan los datos de entrenamiento.  \n",
    "La red generadora se encarga de generar nuevas instancias del mismo dominio que el del conjunto de datos de origen y la red discriminadora se encarga de discriminar si los datos de entrada son reales, es decir pertenecientes al conjunto de datos de entrada o si son ficticios, es decir generador artificialmente. Ambas redes se entrenan de manera de manera conjunta de manera que G maximice sus posibilidades de no ser detectada por D y D de forma que haga cada vez más sofisticados sus métodos de detección de los datos generados artificialmente por G.\n",
    "Estas dos redes adversarias compiten en un juego de suma cero en el que se hipotetiza que eventualmente llegan a un equilibrio de Nash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Modelo Discriminador\n",
    "\n",
    "**1. ¿Qué función de pérdida utiliza el discriminador? ¿Por qué? :**\n",
    "\n",
    "El discriminador utiliza la función de pérdida: binary_crossentropy, la cual es adecuada para tareas de clasificación binaria, lo cual es justamente el objetivo del discriminador en una GAN.\n",
    "Su rol es distinguir entre datos reales y falsos. Toma muestras de entrada y predice la probabilidad de que la entrada sea real o falsa, lo cual es una tarea de clasificación entre dos clases: real (1) y falsa (0). \n",
    "La función de pérdida binary_crossentropy mide la diferencia entre las probabilidades predichas y las etiquetas verdaderas. Calcula la pérdida comparando la salida pronosticada con las etiquetas binarias verdaderas (0,1) y penaliza el modelo en función de la diferencia entre ambas.\n",
    "\n",
    "**2. Busca en la bibliografía la razón por la que se propone utilizar `RMSProp` como optimizador en vez de otros.**\n",
    "\n",
    "La razón por la que se utiliza este optimizador es porque proporciona una técnica para evitar el problema de explosión o desaparición del gradiente, mediante el uso de un promedio móvil de gradientes cuadrados para normalizar el gradiente. Esta normalización ayuda a mantener un tamaño de paso equilibrado durante la optimización, siendo el paso lo que determina cuánto se actualizan los parámetros de cada iteración. Un tamaño de paso más grande equivale a una actualización de parámetros más significa, lo que significa que los parámetros cambiarán más rápido. Por el contrario, un tamaño de paso más pequeño conduce a actualizaciones más graduales. RMSProp, adapta el tamaño del paso para cada parámetro en función de la magnitud de sus gradientes, lo cual ayuda a equilibrar las actualizaciones. Específicamente, disminuye el tamaño de paso para gradientes grandes,  evitando que exploten, mientras que aumenta el tamaño de paso para gradientes pequeños para evitar que desaparezcan. \n",
    "Entonces, RMSProp no trata la tasa de aprendizaje como un parámetro fijo sino que la adapta dinámicamiente a medida que avanza la optimización. Al ajustar la tasa de aprendizaje en función de las características de los gradientes, RMSProp tiene como objetivo mejorar la estabilidad y la eficiencia del proceso de entrenamiento.\n",
    "\n",
    "**3. ¿Cuál es la razón de utilizar decay?**\n",
    "\n",
    "El parámetro decay se refiere a la tasa de caida aplicada al promedio móvil de los gradientes al cuadrado. Controla la velocidad a la que se actualiza ese promedio móvil. Un valor más pequeño permite que el optimizador reacciones más rápido a los gradientes nuevos, mientras que un valor más grande hace que el optimizador dependa más de los gradientes anteriores.\n",
    "Es decir, el propósito de usar este parámetro en el optimizador RMSProp es controlar la influencia de los gradientes anteriores en la actualización actual. Afecta la adaptabilidad de la tasa de aprendizaje al determinar cuánto peso de le da a los gradientes recientes frente a los anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:02.044829Z",
     "iopub.status.busy": "2023-06-08T12:18:02.043203Z",
     "iopub.status.idle": "2023-06-08T12:18:02.176181Z",
     "shell.execute_reply": "2023-06-08T12:18:02.175171Z",
     "shell.execute_reply.started": "2023-06-08T12:18:02.044792Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = discriminator_model()\n",
    "discriminator.compile(loss='binary_crossentropy', \n",
    "                      optimizer=RMSprop(lr=0.0002, decay=6e-8), \n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:02.183081Z",
     "iopub.status.busy": "2023-06-08T12:18:02.180960Z",
     "iopub.status.idle": "2023-06-08T12:18:02.419574Z",
     "shell.execute_reply": "2023-06-08T12:18:02.417673Z",
     "shell.execute_reply.started": "2023-06-08T12:18:02.183046Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = generator_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Modelo adversario\n",
    "\n",
    "El modelo adversario es únicamente el generador-discriminador apilados juntos. Los parámetros de entrenamiento son los mismos que en el modelo Discriminador, salvo por una tasa de aprendizaje reducida y la correspondiente disminución del peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:03.191386Z",
     "iopub.status.busy": "2023-06-08T12:18:03.191028Z",
     "iopub.status.idle": "2023-06-08T12:18:03.199847Z",
     "shell.execute_reply": "2023-06-08T12:18:03.198843Z",
     "shell.execute_reply.started": "2023-06-08T12:18:03.191357Z"
    }
   },
   "outputs": [],
   "source": [
    "def adversarial_model():\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=RMSprop(lr=0.0001, decay=3e-8), \n",
    "                  metrics=['accuracy'])\n",
    "    discriminator.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:53.955856Z",
     "iopub.status.busy": "2023-06-08T12:18:53.955443Z",
     "iopub.status.idle": "2023-06-08T12:18:54.082915Z",
     "shell.execute_reply": "2023-06-08T12:18:54.081995Z",
     "shell.execute_reply.started": "2023-06-08T12:18:53.955825Z"
    }
   },
   "outputs": [],
   "source": [
    "adversarial = adversarial_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:55.214520Z",
     "iopub.status.busy": "2023-06-08T12:18:55.214152Z",
     "iopub.status.idle": "2023-06-08T12:18:55.227440Z",
     "shell.execute_reply": "2023-06-08T12:18:55.226535Z",
     "shell.execute_reply.started": "2023-06-08T12:18:55.214491Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images(saveToFile=False, fake=True, samples=16, noise=None, epoch=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, latent_dim])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % epoch\n",
    "        images = generator.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [img_rows, img_cols])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if saveToFile:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero determinamos si el modelo de discriminador es correcto entrenándolo solo con imágenes reales y falsas. Después, los modelos Discriminador y Adversario entrenan uno tras otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:18:57.279071Z",
     "iopub.status.busy": "2023-06-08T12:18:57.278664Z",
     "iopub.status.idle": "2023-06-08T12:18:57.290109Z",
     "shell.execute_reply": "2023-06-08T12:18:57.288964Z",
     "shell.execute_reply.started": "2023-06-08T12:18:57.279039Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_epochs=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_dim])\n",
    "        for epoch in range(train_epochs):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # select a random half of images\n",
    "            images_train = x_train[np.random.randint(0, x_train.shape[0], size=batch_size), :, :, :]\n",
    "            \n",
    "            # sample noise and generate a batch of new images\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
    "            images_fake = generator.predict(noise)\n",
    "            \n",
    "            # train the discriminator (real classified as ones and generated as zeros)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = discriminator.train_on_batch(x, y)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            \n",
    "            # train the generator (wants discriminator to mistake images as real)\n",
    "            y = np.ones([batch_size, 1])\n",
    "            a_loss = adversarial.train_on_batch(noise, y)\n",
    "            \n",
    "            log_msg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
    "            log_msg = \"%s  [A loss: %f, acc: %f]\" % (log_msg, a_loss[0], a_loss[1])\n",
    "            print(log_msg)\n",
    "            if save_interval>0:\n",
    "                if (epoch+1)%save_interval==0:\n",
    "                    plot_images(saveToFile=True, samples=noise_input.shape[0],\n",
    "                                noise=noise_input, epoch=(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1. ¿Cuál es la finalidad de `noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])`? ¿Por qué estas dimensiones?**\n",
    "\n",
    "La finalidad de esa línea es generar vectores de ruido aleatorio que sirven como entrada al modelo generados durante el entrenamiento. En las redes adversas generativas, el modelo generador toma ruido aleatorio como entrada y genera muestras sintéticas que se asemejan a los datos reales. El ruido actúa como una fuente de aleatoriedad que le permite al generador producir salidas diversas y variadas.\n",
    "\n",
    "En este caso, se generan números aleatorios a partir de una distribución uniforme entre -1 y 1. La matriz de ruido resultante tiene una forma de: [batch_size, latent_dim], donde batch_size corresponde al número de vectores de ruido generados y laten_dim representa la dimensionalidad de los vectores de ruido.\n",
    "\n",
    "El generador recibe un número batch_size de vectores de ruido, cada uno de estos vectores con dimensionalidad latent_dim, y los utiliza para generar un lote de muestras sintéticas.\n",
    "Al generar estas muestras a partir de una distribución uniforme continua en el rango de -1 a 1, el modelo garantiza que la entrada al generador cubra un espacio amplio y continuo. ayudando a garantizar la diversidad en las muestras generadas.\n",
    "\n",
    "**2. ¿Cuál es la finalidad de `images_fake = generator.predict(noise)`?**\n",
    "\n",
    "La finalidad de esa linea es la generación de imágenes sintéticas aplicando el modelo del generados a la entrada de ruido que mencionamos antes. La función generator.predict() aplica el modelo del generador a la entrada de ruido que se proporciona y produce las salidas correspondientes que son las muestras sintéticas.\n",
    "La matriz images_fake generada contiene estas nuevas imágenes generadas por el generador y tiene dimensiones: [batch_size, image_height, image_width, image_channels] donde batch_size representa el número de imágenes sintéticas generadas, y los demás corresponden a las dimensiones de las imágenes. \n",
    "\n",
    "**3. ¿Cuál es la finalidad del código que sigue?**\n",
    "```python\n",
    "x = np.concatenate((images_train, images_fake))\n",
    "y = np.ondas([2*batch_size, 1])\n",
    "y[batch_size:, :] = 0\n",
    "```\n",
    "\n",
    "El objetivo del código es la generación de los datos de entrenamiento para el discriminador combinando imagenes reales con las imagenes generadas por el generador y la asignación de las etiquetas correspondientes, ya que el discriminador actúa como un clasificador (entre real o falso).\n",
    "En primer lugar se genera el array x de datos, el cual tendrá un tamaño de (2*batch_size, image_height, image_width, image_channels), ya que primero contiene todas las imágenes reales y luego las generadas por el generador.\n",
    "Luego se genera el array y de unos, es decir todas las etiquetas están seteadas a 1 en este punto, indicando que todas las imagenes son reales.\n",
    "En la última línea, se asigna la etiqueta 0 (falso) a la segunda mitad del array, que corresponde a las imágenes falsas generadas por el generador.\n",
    "\n",
    "**4. ¿Qué realiza el comando `d_loss = discriminator.train_on_batch(x, y)`? ¿Qué devuelve?**\n",
    "\n",
    "Esa linea se usa para realizar un paso de actualización en el modelo discriminador usando los datos creados anteriormente x e y. Calcula la pérdida y actualiza los pesos del modelo en función de los gradientes calculados. \n",
    "\n",
    "La función train_on_batch devuelve una lista de dos valores:\n",
    "El primero d_loss[0] representa el valor de pérdida calculado para el lote actual de muestras de entrenamiento Indica que tan bien se desempeña el discriminador en distinguir entre imagenes reales y falsas.\n",
    "El segundo valor d_loss[1] representa la precisión del discriminador en el lote actual de muestras de entrenamiento. Indica el porcentaje de muestras correctamente clasificadas.\n",
    "\n",
    "**5. ¿Qué realiza el comando `a_loss = adversarial.train_on_batch(noise, y)`? ¿Qué devuelve?**\n",
    "\n",
    "Este comando se utiliza para hacer un paso de actualización en el modelo adversario (combinación de generador y discriminador) usando el ruido de entrada proporcionado y las etiquetas. \n",
    "Calcula la pérdida y actualiza los pesos del modelo en función de los gradientes calculados.\n",
    "\n",
    "La función train_on_batch devuelve los mismos valores mencionados arriba: \n",
    "a_loss[0] representa el valor de pérdida calculado para el lote actual de muestras de ruido. Indica qué tan bien se desempeña el modelo adversario en la generación de imágenes realistas que pueden engañar al discriminador.\n",
    "a_loss[1] representa la precisión del modelo adversario en el lote actual. Indica la proporción de muestras correctamente clasificadas (generadas como reales) por el discriminador. Una precisión más alta para el modelo adversario significa que está generando imágenes que son más realistas y pueden engañar mejor al discriminador para que las clasifique como reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:19:01.957596Z",
     "iopub.status.busy": "2023-06-08T12:19:01.957223Z",
     "iopub.status.idle": "2023-06-08T12:19:01.964523Z",
     "shell.execute_reply": "2023-06-08T12:19:01.963556Z",
     "shell.execute_reply.started": "2023-06-08T12:19:01.957564Z"
    }
   },
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:20:06.806061Z",
     "iopub.status.busy": "2023-06-08T12:20:06.805492Z",
     "iopub.status.idle": "2023-06-08T12:23:50.257929Z",
     "shell.execute_reply": "2023-06-08T12:23:50.256930Z",
     "shell.execute_reply.started": "2023-06-08T12:20:06.806018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n",
      "0: [D loss: 0.641600, acc: 0.589844]  [A loss: 0.643688, acc: 0.644531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "1: [D loss: 0.648341, acc: 0.580078]  [A loss: 1.159408, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "2: [D loss: 0.632910, acc: 0.636719]  [A loss: 0.765706, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "3: [D loss: 0.599605, acc: 0.697266]  [A loss: 1.058722, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "4: [D loss: 0.611533, acc: 0.666016]  [A loss: 0.715908, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "5: [D loss: 0.603328, acc: 0.683594]  [A loss: 1.331691, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "6: [D loss: 0.626022, acc: 0.615234]  [A loss: 0.526454, acc: 0.808594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "7: [D loss: 0.687756, acc: 0.566406]  [A loss: 1.418407, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "8: [D loss: 0.702641, acc: 0.523438]  [A loss: 0.687177, acc: 0.546875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "9: [D loss: 0.666148, acc: 0.587891]  [A loss: 1.000296, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "10: [D loss: 0.597506, acc: 0.714844]  [A loss: 0.780039, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "11: [D loss: 0.599524, acc: 0.689453]  [A loss: 1.051429, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "12: [D loss: 0.591843, acc: 0.701172]  [A loss: 0.728360, acc: 0.476562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "13: [D loss: 0.593846, acc: 0.677734]  [A loss: 1.166678, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "14: [D loss: 0.622112, acc: 0.642578]  [A loss: 0.561638, acc: 0.742188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "15: [D loss: 0.666581, acc: 0.570312]  [A loss: 1.468988, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "16: [D loss: 0.701862, acc: 0.517578]  [A loss: 0.630597, acc: 0.664062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "17: [D loss: 0.657339, acc: 0.560547]  [A loss: 1.095716, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "18: [D loss: 0.615343, acc: 0.664062]  [A loss: 0.694583, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "19: [D loss: 0.602650, acc: 0.666016]  [A loss: 1.089680, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "20: [D loss: 0.605909, acc: 0.660156]  [A loss: 0.645971, acc: 0.617188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "21: [D loss: 0.614939, acc: 0.662109]  [A loss: 1.097324, acc: 0.062500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "22: [D loss: 0.623472, acc: 0.638672]  [A loss: 0.676806, acc: 0.597656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "23: [D loss: 0.632841, acc: 0.615234]  [A loss: 1.172166, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "24: [D loss: 0.631398, acc: 0.630859]  [A loss: 0.609858, acc: 0.664062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "25: [D loss: 0.628178, acc: 0.607422]  [A loss: 1.313073, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "26: [D loss: 0.670754, acc: 0.570312]  [A loss: 0.607990, acc: 0.675781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "27: [D loss: 0.631888, acc: 0.580078]  [A loss: 1.184061, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "28: [D loss: 0.625201, acc: 0.621094]  [A loss: 0.640059, acc: 0.625000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "29: [D loss: 0.632741, acc: 0.628906]  [A loss: 1.163954, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "30: [D loss: 0.639835, acc: 0.595703]  [A loss: 0.721514, acc: 0.460938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "31: [D loss: 0.605528, acc: 0.664062]  [A loss: 1.047735, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "32: [D loss: 0.607136, acc: 0.666016]  [A loss: 0.713760, acc: 0.468750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "33: [D loss: 0.616341, acc: 0.666016]  [A loss: 1.123184, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "34: [D loss: 0.596725, acc: 0.671875]  [A loss: 0.662460, acc: 0.597656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "35: [D loss: 0.599692, acc: 0.671875]  [A loss: 1.232867, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "36: [D loss: 0.643153, acc: 0.591797]  [A loss: 0.561937, acc: 0.738281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "37: [D loss: 0.668182, acc: 0.558594]  [A loss: 1.425789, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "38: [D loss: 0.670767, acc: 0.546875]  [A loss: 0.637735, acc: 0.601562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "39: [D loss: 0.625964, acc: 0.640625]  [A loss: 1.046241, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "40: [D loss: 0.635003, acc: 0.628906]  [A loss: 0.785537, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "41: [D loss: 0.605056, acc: 0.685547]  [A loss: 0.966791, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "42: [D loss: 0.598471, acc: 0.683594]  [A loss: 0.800436, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "43: [D loss: 0.599601, acc: 0.699219]  [A loss: 1.118427, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "44: [D loss: 0.617358, acc: 0.636719]  [A loss: 0.634034, acc: 0.640625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "45: [D loss: 0.649270, acc: 0.597656]  [A loss: 1.369697, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "46: [D loss: 0.675345, acc: 0.550781]  [A loss: 0.545511, acc: 0.792969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "47: [D loss: 0.662038, acc: 0.572266]  [A loss: 1.273792, acc: 0.007812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "48: [D loss: 0.670126, acc: 0.548828]  [A loss: 0.745459, acc: 0.464844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "49: [D loss: 0.591917, acc: 0.695312]  [A loss: 0.942010, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "50: [D loss: 0.611365, acc: 0.687500]  [A loss: 0.752272, acc: 0.441406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "51: [D loss: 0.596904, acc: 0.710938]  [A loss: 1.012498, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "52: [D loss: 0.593405, acc: 0.722656]  [A loss: 0.746760, acc: 0.421875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "53: [D loss: 0.610189, acc: 0.689453]  [A loss: 1.091241, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "54: [D loss: 0.612101, acc: 0.656250]  [A loss: 0.602157, acc: 0.667969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "55: [D loss: 0.640247, acc: 0.611328]  [A loss: 1.462125, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "56: [D loss: 0.685847, acc: 0.541016]  [A loss: 0.572798, acc: 0.734375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "57: [D loss: 0.665813, acc: 0.550781]  [A loss: 1.219858, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "58: [D loss: 0.654804, acc: 0.568359]  [A loss: 0.732457, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "59: [D loss: 0.612808, acc: 0.673828]  [A loss: 0.954019, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "60: [D loss: 0.610513, acc: 0.673828]  [A loss: 0.754719, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "61: [D loss: 0.592952, acc: 0.720703]  [A loss: 1.097361, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "62: [D loss: 0.610333, acc: 0.666016]  [A loss: 0.699795, acc: 0.527344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "63: [D loss: 0.611281, acc: 0.662109]  [A loss: 1.153499, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "64: [D loss: 0.621419, acc: 0.666016]  [A loss: 0.582223, acc: 0.699219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "65: [D loss: 0.645886, acc: 0.617188]  [A loss: 1.405917, acc: 0.003906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "66: [D loss: 0.694424, acc: 0.552734]  [A loss: 0.633628, acc: 0.636719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "67: [D loss: 0.648515, acc: 0.587891]  [A loss: 1.076590, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "68: [D loss: 0.620578, acc: 0.650391]  [A loss: 0.725640, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "69: [D loss: 0.611460, acc: 0.667969]  [A loss: 0.936214, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "70: [D loss: 0.614095, acc: 0.693359]  [A loss: 0.799926, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "71: [D loss: 0.600844, acc: 0.699219]  [A loss: 0.934789, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "72: [D loss: 0.599609, acc: 0.699219]  [A loss: 0.725699, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "73: [D loss: 0.634000, acc: 0.617188]  [A loss: 1.201418, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "74: [D loss: 0.637284, acc: 0.632812]  [A loss: 0.545416, acc: 0.777344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "75: [D loss: 0.662815, acc: 0.580078]  [A loss: 1.534291, acc: 0.000000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "76: [D loss: 0.737793, acc: 0.515625]  [A loss: 0.655330, acc: 0.601562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "77: [D loss: 0.629303, acc: 0.623047]  [A loss: 0.996156, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "78: [D loss: 0.603761, acc: 0.705078]  [A loss: 0.759791, acc: 0.421875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "79: [D loss: 0.616639, acc: 0.660156]  [A loss: 1.074740, acc: 0.062500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "80: [D loss: 0.626848, acc: 0.652344]  [A loss: 0.716148, acc: 0.507812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "81: [D loss: 0.600745, acc: 0.689453]  [A loss: 1.054266, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "82: [D loss: 0.621593, acc: 0.685547]  [A loss: 0.717880, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "83: [D loss: 0.610387, acc: 0.677734]  [A loss: 1.141279, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "84: [D loss: 0.637530, acc: 0.615234]  [A loss: 0.666308, acc: 0.546875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "85: [D loss: 0.620755, acc: 0.650391]  [A loss: 1.247843, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "86: [D loss: 0.665728, acc: 0.562500]  [A loss: 0.611586, acc: 0.683594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "87: [D loss: 0.663558, acc: 0.589844]  [A loss: 1.152631, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "88: [D loss: 0.670933, acc: 0.583984]  [A loss: 0.789776, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "89: [D loss: 0.627175, acc: 0.656250]  [A loss: 0.968496, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "90: [D loss: 0.614286, acc: 0.679688]  [A loss: 0.759648, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "91: [D loss: 0.610818, acc: 0.677734]  [A loss: 1.069216, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "92: [D loss: 0.621808, acc: 0.634766]  [A loss: 0.715483, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "93: [D loss: 0.620097, acc: 0.652344]  [A loss: 1.189815, acc: 0.027344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "94: [D loss: 0.626857, acc: 0.613281]  [A loss: 0.619344, acc: 0.640625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "95: [D loss: 0.650039, acc: 0.585938]  [A loss: 1.252070, acc: 0.019531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "96: [D loss: 0.696449, acc: 0.546875]  [A loss: 0.689459, acc: 0.519531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "97: [D loss: 0.651100, acc: 0.607422]  [A loss: 0.993267, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "98: [D loss: 0.621008, acc: 0.667969]  [A loss: 0.743042, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "99: [D loss: 0.638525, acc: 0.646484]  [A loss: 1.069809, acc: 0.101562]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "100: [D loss: 0.624187, acc: 0.640625]  [A loss: 0.756318, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "101: [D loss: 0.629006, acc: 0.671875]  [A loss: 0.972612, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "102: [D loss: 0.615237, acc: 0.679688]  [A loss: 0.793774, acc: 0.363281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "103: [D loss: 0.619905, acc: 0.666016]  [A loss: 1.022826, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "104: [D loss: 0.640607, acc: 0.630859]  [A loss: 0.619227, acc: 0.660156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "105: [D loss: 0.635547, acc: 0.623047]  [A loss: 1.266111, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "106: [D loss: 0.655722, acc: 0.591797]  [A loss: 0.627754, acc: 0.636719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "107: [D loss: 0.671580, acc: 0.556641]  [A loss: 1.143068, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "108: [D loss: 0.646775, acc: 0.603516]  [A loss: 0.726053, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "109: [D loss: 0.629043, acc: 0.625000]  [A loss: 0.953342, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "110: [D loss: 0.601464, acc: 0.701172]  [A loss: 0.742902, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "111: [D loss: 0.617616, acc: 0.664062]  [A loss: 1.071224, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "112: [D loss: 0.647697, acc: 0.626953]  [A loss: 0.686542, acc: 0.507812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "113: [D loss: 0.649737, acc: 0.625000]  [A loss: 1.198037, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "114: [D loss: 0.669379, acc: 0.597656]  [A loss: 0.717104, acc: 0.472656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "115: [D loss: 0.640628, acc: 0.621094]  [A loss: 1.060741, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "116: [D loss: 0.639831, acc: 0.634766]  [A loss: 0.713603, acc: 0.523438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "117: [D loss: 0.615095, acc: 0.654297]  [A loss: 1.074726, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "118: [D loss: 0.648261, acc: 0.615234]  [A loss: 0.723353, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "119: [D loss: 0.624282, acc: 0.650391]  [A loss: 1.044978, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "120: [D loss: 0.635711, acc: 0.611328]  [A loss: 0.657918, acc: 0.554688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "121: [D loss: 0.647270, acc: 0.650391]  [A loss: 1.110553, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "122: [D loss: 0.656245, acc: 0.609375]  [A loss: 0.697750, acc: 0.550781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "123: [D loss: 0.652610, acc: 0.589844]  [A loss: 1.118340, acc: 0.062500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "124: [D loss: 0.637378, acc: 0.609375]  [A loss: 0.664268, acc: 0.570312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "125: [D loss: 0.626665, acc: 0.664062]  [A loss: 1.225897, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "126: [D loss: 0.654403, acc: 0.574219]  [A loss: 0.696556, acc: 0.527344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "127: [D loss: 0.638659, acc: 0.619141]  [A loss: 1.089496, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "128: [D loss: 0.654661, acc: 0.603516]  [A loss: 0.702109, acc: 0.523438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "129: [D loss: 0.626537, acc: 0.615234]  [A loss: 1.016944, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "130: [D loss: 0.625487, acc: 0.658203]  [A loss: 0.723845, acc: 0.472656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "131: [D loss: 0.631087, acc: 0.628906]  [A loss: 1.067082, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "132: [D loss: 0.643556, acc: 0.617188]  [A loss: 0.727497, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "133: [D loss: 0.642916, acc: 0.617188]  [A loss: 1.037184, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "134: [D loss: 0.630937, acc: 0.634766]  [A loss: 0.697338, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "135: [D loss: 0.644549, acc: 0.619141]  [A loss: 1.152329, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "136: [D loss: 0.656447, acc: 0.585938]  [A loss: 0.683092, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "137: [D loss: 0.656364, acc: 0.621094]  [A loss: 1.100456, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "138: [D loss: 0.630285, acc: 0.619141]  [A loss: 0.722973, acc: 0.476562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "139: [D loss: 0.626372, acc: 0.630859]  [A loss: 1.060730, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "140: [D loss: 0.640576, acc: 0.623047]  [A loss: 0.703084, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "141: [D loss: 0.656122, acc: 0.623047]  [A loss: 1.085078, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "142: [D loss: 0.655869, acc: 0.607422]  [A loss: 0.722501, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "143: [D loss: 0.646242, acc: 0.623047]  [A loss: 1.018598, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "144: [D loss: 0.637645, acc: 0.617188]  [A loss: 0.721857, acc: 0.468750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "145: [D loss: 0.622081, acc: 0.628906]  [A loss: 0.976149, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "146: [D loss: 0.638357, acc: 0.642578]  [A loss: 0.768281, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "147: [D loss: 0.630465, acc: 0.654297]  [A loss: 1.035452, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "148: [D loss: 0.644039, acc: 0.638672]  [A loss: 0.687957, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "149: [D loss: 0.664489, acc: 0.585938]  [A loss: 1.181412, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "150: [D loss: 0.661268, acc: 0.560547]  [A loss: 0.691380, acc: 0.546875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "151: [D loss: 0.636509, acc: 0.599609]  [A loss: 1.049415, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "152: [D loss: 0.636117, acc: 0.628906]  [A loss: 0.729275, acc: 0.480469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "153: [D loss: 0.638046, acc: 0.632812]  [A loss: 1.036239, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "154: [D loss: 0.634901, acc: 0.658203]  [A loss: 0.731230, acc: 0.468750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "155: [D loss: 0.634886, acc: 0.638672]  [A loss: 1.003652, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "156: [D loss: 0.630963, acc: 0.662109]  [A loss: 0.729579, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "157: [D loss: 0.636656, acc: 0.625000]  [A loss: 1.103460, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "158: [D loss: 0.625019, acc: 0.636719]  [A loss: 0.653403, acc: 0.617188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "159: [D loss: 0.640554, acc: 0.597656]  [A loss: 1.207433, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "160: [D loss: 0.664526, acc: 0.574219]  [A loss: 0.672660, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "161: [D loss: 0.657050, acc: 0.578125]  [A loss: 1.006833, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "162: [D loss: 0.633496, acc: 0.634766]  [A loss: 0.737624, acc: 0.472656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "163: [D loss: 0.665012, acc: 0.593750]  [A loss: 0.936816, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "164: [D loss: 0.639229, acc: 0.603516]  [A loss: 0.791184, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "165: [D loss: 0.621929, acc: 0.671875]  [A loss: 1.029582, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "166: [D loss: 0.626281, acc: 0.654297]  [A loss: 0.720822, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "167: [D loss: 0.648216, acc: 0.607422]  [A loss: 1.107067, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "168: [D loss: 0.651878, acc: 0.595703]  [A loss: 0.690713, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "169: [D loss: 0.644256, acc: 0.611328]  [A loss: 1.086924, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "170: [D loss: 0.653108, acc: 0.587891]  [A loss: 0.737724, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "171: [D loss: 0.640017, acc: 0.636719]  [A loss: 0.999781, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "172: [D loss: 0.643209, acc: 0.619141]  [A loss: 0.744016, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "173: [D loss: 0.613703, acc: 0.681641]  [A loss: 0.959529, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "174: [D loss: 0.633541, acc: 0.640625]  [A loss: 0.736156, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "175: [D loss: 0.647389, acc: 0.636719]  [A loss: 1.075251, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "176: [D loss: 0.642294, acc: 0.617188]  [A loss: 0.723352, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "177: [D loss: 0.625899, acc: 0.638672]  [A loss: 1.055280, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "178: [D loss: 0.626320, acc: 0.656250]  [A loss: 0.688970, acc: 0.527344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "179: [D loss: 0.639431, acc: 0.652344]  [A loss: 1.101415, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "180: [D loss: 0.646127, acc: 0.609375]  [A loss: 0.667529, acc: 0.589844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "181: [D loss: 0.620728, acc: 0.648438]  [A loss: 1.106327, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "182: [D loss: 0.640112, acc: 0.644531]  [A loss: 0.693180, acc: 0.550781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "183: [D loss: 0.639047, acc: 0.605469]  [A loss: 1.102365, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "184: [D loss: 0.665287, acc: 0.585938]  [A loss: 0.706454, acc: 0.539062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "185: [D loss: 0.631079, acc: 0.636719]  [A loss: 0.963920, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "186: [D loss: 0.637902, acc: 0.656250]  [A loss: 0.796338, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "187: [D loss: 0.630938, acc: 0.630859]  [A loss: 0.922536, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "188: [D loss: 0.613480, acc: 0.667969]  [A loss: 0.835662, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "189: [D loss: 0.641697, acc: 0.626953]  [A loss: 0.926572, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "190: [D loss: 0.605849, acc: 0.708984]  [A loss: 0.839989, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "191: [D loss: 0.631287, acc: 0.664062]  [A loss: 0.998258, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "192: [D loss: 0.623308, acc: 0.666016]  [A loss: 0.639939, acc: 0.578125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "193: [D loss: 0.670056, acc: 0.548828]  [A loss: 1.310170, acc: 0.011719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "194: [D loss: 0.703788, acc: 0.546875]  [A loss: 0.606317, acc: 0.695312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "195: [D loss: 0.665794, acc: 0.564453]  [A loss: 1.052436, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "196: [D loss: 0.667238, acc: 0.568359]  [A loss: 0.760339, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "197: [D loss: 0.629269, acc: 0.642578]  [A loss: 0.863786, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "198: [D loss: 0.612310, acc: 0.675781]  [A loss: 0.856891, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "199: [D loss: 0.634319, acc: 0.650391]  [A loss: 0.893617, acc: 0.238281]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "200: [D loss: 0.637594, acc: 0.654297]  [A loss: 0.794658, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "201: [D loss: 0.622369, acc: 0.654297]  [A loss: 0.904552, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "202: [D loss: 0.620048, acc: 0.646484]  [A loss: 0.841103, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "203: [D loss: 0.646608, acc: 0.632812]  [A loss: 0.938251, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "204: [D loss: 0.616697, acc: 0.671875]  [A loss: 0.816607, acc: 0.343750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "205: [D loss: 0.600975, acc: 0.697266]  [A loss: 0.972221, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "206: [D loss: 0.632815, acc: 0.658203]  [A loss: 0.813250, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "207: [D loss: 0.628610, acc: 0.638672]  [A loss: 1.026342, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "208: [D loss: 0.630649, acc: 0.617188]  [A loss: 0.716488, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "209: [D loss: 0.609012, acc: 0.667969]  [A loss: 1.153495, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "210: [D loss: 0.641674, acc: 0.613281]  [A loss: 0.666992, acc: 0.593750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "211: [D loss: 0.648345, acc: 0.638672]  [A loss: 1.256452, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "212: [D loss: 0.682725, acc: 0.568359]  [A loss: 0.682941, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "213: [D loss: 0.645303, acc: 0.595703]  [A loss: 1.073898, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "214: [D loss: 0.639727, acc: 0.630859]  [A loss: 0.745574, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "215: [D loss: 0.629267, acc: 0.650391]  [A loss: 1.030323, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "216: [D loss: 0.639387, acc: 0.632812]  [A loss: 0.758046, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "217: [D loss: 0.640739, acc: 0.617188]  [A loss: 1.042336, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "218: [D loss: 0.658442, acc: 0.611328]  [A loss: 0.777433, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "219: [D loss: 0.628766, acc: 0.664062]  [A loss: 1.010699, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "220: [D loss: 0.633279, acc: 0.640625]  [A loss: 0.725085, acc: 0.441406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "221: [D loss: 0.647144, acc: 0.617188]  [A loss: 1.056356, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "222: [D loss: 0.618012, acc: 0.660156]  [A loss: 0.763891, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "223: [D loss: 0.630314, acc: 0.628906]  [A loss: 1.053765, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "224: [D loss: 0.632533, acc: 0.640625]  [A loss: 0.707544, acc: 0.507812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "225: [D loss: 0.636622, acc: 0.630859]  [A loss: 1.087657, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "226: [D loss: 0.643648, acc: 0.625000]  [A loss: 0.691505, acc: 0.535156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "227: [D loss: 0.655191, acc: 0.595703]  [A loss: 1.097680, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "228: [D loss: 0.653982, acc: 0.603516]  [A loss: 0.747141, acc: 0.468750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "229: [D loss: 0.635591, acc: 0.634766]  [A loss: 0.984136, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "230: [D loss: 0.632347, acc: 0.621094]  [A loss: 0.744143, acc: 0.468750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "231: [D loss: 0.640831, acc: 0.611328]  [A loss: 0.963018, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "232: [D loss: 0.637003, acc: 0.621094]  [A loss: 0.794293, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "233: [D loss: 0.635126, acc: 0.650391]  [A loss: 1.006737, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "234: [D loss: 0.638324, acc: 0.626953]  [A loss: 0.736613, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "235: [D loss: 0.649057, acc: 0.609375]  [A loss: 0.995854, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "236: [D loss: 0.632173, acc: 0.662109]  [A loss: 0.742255, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "237: [D loss: 0.608474, acc: 0.681641]  [A loss: 1.058226, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "238: [D loss: 0.652032, acc: 0.617188]  [A loss: 0.726258, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "239: [D loss: 0.616922, acc: 0.652344]  [A loss: 1.138934, acc: 0.062500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "240: [D loss: 0.634737, acc: 0.626953]  [A loss: 0.698708, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "241: [D loss: 0.634923, acc: 0.628906]  [A loss: 1.106260, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "242: [D loss: 0.640937, acc: 0.613281]  [A loss: 0.672487, acc: 0.570312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "243: [D loss: 0.622135, acc: 0.652344]  [A loss: 1.060334, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "244: [D loss: 0.639357, acc: 0.628906]  [A loss: 0.746203, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "245: [D loss: 0.639572, acc: 0.625000]  [A loss: 0.974143, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "246: [D loss: 0.624069, acc: 0.658203]  [A loss: 0.786602, acc: 0.414062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "247: [D loss: 0.624590, acc: 0.669922]  [A loss: 0.957238, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "248: [D loss: 0.631477, acc: 0.640625]  [A loss: 0.793014, acc: 0.378906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "249: [D loss: 0.630244, acc: 0.630859]  [A loss: 1.040465, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "250: [D loss: 0.626387, acc: 0.644531]  [A loss: 0.712429, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "251: [D loss: 0.634841, acc: 0.628906]  [A loss: 1.106306, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "252: [D loss: 0.629467, acc: 0.623047]  [A loss: 0.715876, acc: 0.488281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "253: [D loss: 0.645600, acc: 0.625000]  [A loss: 1.054396, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "254: [D loss: 0.618625, acc: 0.669922]  [A loss: 0.705579, acc: 0.523438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "255: [D loss: 0.628938, acc: 0.625000]  [A loss: 1.098944, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "256: [D loss: 0.627322, acc: 0.630859]  [A loss: 0.744617, acc: 0.445312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "257: [D loss: 0.640934, acc: 0.625000]  [A loss: 1.067195, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "258: [D loss: 0.648915, acc: 0.619141]  [A loss: 0.796033, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "259: [D loss: 0.652617, acc: 0.607422]  [A loss: 1.117998, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "260: [D loss: 0.641004, acc: 0.615234]  [A loss: 0.735546, acc: 0.476562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "261: [D loss: 0.644420, acc: 0.636719]  [A loss: 1.095639, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "262: [D loss: 0.642973, acc: 0.605469]  [A loss: 0.788806, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "263: [D loss: 0.626978, acc: 0.646484]  [A loss: 1.003200, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "264: [D loss: 0.619757, acc: 0.654297]  [A loss: 0.833180, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "265: [D loss: 0.627950, acc: 0.642578]  [A loss: 0.964763, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "266: [D loss: 0.610488, acc: 0.673828]  [A loss: 0.824327, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "267: [D loss: 0.642753, acc: 0.619141]  [A loss: 1.047509, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "268: [D loss: 0.624665, acc: 0.652344]  [A loss: 0.752159, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "269: [D loss: 0.628087, acc: 0.644531]  [A loss: 1.135379, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "270: [D loss: 0.650198, acc: 0.615234]  [A loss: 0.659784, acc: 0.585938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "271: [D loss: 0.653996, acc: 0.566406]  [A loss: 1.073021, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "272: [D loss: 0.656489, acc: 0.601562]  [A loss: 0.695802, acc: 0.492188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "273: [D loss: 0.641509, acc: 0.619141]  [A loss: 1.066349, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "274: [D loss: 0.634692, acc: 0.638672]  [A loss: 0.795899, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "275: [D loss: 0.636929, acc: 0.632812]  [A loss: 0.936139, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "276: [D loss: 0.639599, acc: 0.628906]  [A loss: 0.826074, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "277: [D loss: 0.615664, acc: 0.677734]  [A loss: 0.975375, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "278: [D loss: 0.614762, acc: 0.669922]  [A loss: 0.910341, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "279: [D loss: 0.614954, acc: 0.689453]  [A loss: 0.909822, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "280: [D loss: 0.632599, acc: 0.634766]  [A loss: 0.938289, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "281: [D loss: 0.605045, acc: 0.685547]  [A loss: 0.861797, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "282: [D loss: 0.637680, acc: 0.623047]  [A loss: 1.066280, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "283: [D loss: 0.651138, acc: 0.621094]  [A loss: 0.664293, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "284: [D loss: 0.624790, acc: 0.623047]  [A loss: 1.277189, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "285: [D loss: 0.691047, acc: 0.576172]  [A loss: 0.684734, acc: 0.550781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "286: [D loss: 0.646543, acc: 0.593750]  [A loss: 1.078830, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "287: [D loss: 0.644196, acc: 0.617188]  [A loss: 0.795868, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "288: [D loss: 0.628470, acc: 0.667969]  [A loss: 0.998930, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "289: [D loss: 0.631453, acc: 0.638672]  [A loss: 0.829561, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "290: [D loss: 0.629965, acc: 0.664062]  [A loss: 0.931983, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "291: [D loss: 0.617760, acc: 0.636719]  [A loss: 0.834234, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "292: [D loss: 0.624370, acc: 0.652344]  [A loss: 0.920163, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "293: [D loss: 0.638281, acc: 0.640625]  [A loss: 0.864032, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "294: [D loss: 0.644178, acc: 0.630859]  [A loss: 0.923414, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "295: [D loss: 0.636392, acc: 0.642578]  [A loss: 0.833001, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "296: [D loss: 0.640199, acc: 0.666016]  [A loss: 1.005917, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "297: [D loss: 0.616558, acc: 0.677734]  [A loss: 0.765241, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "298: [D loss: 0.636533, acc: 0.640625]  [A loss: 1.010579, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "299: [D loss: 0.617482, acc: 0.632812]  [A loss: 0.803525, acc: 0.382812]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "300: [D loss: 0.649209, acc: 0.597656]  [A loss: 1.083693, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "301: [D loss: 0.669121, acc: 0.619141]  [A loss: 0.650040, acc: 0.636719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "302: [D loss: 0.655333, acc: 0.583984]  [A loss: 1.169513, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "303: [D loss: 0.657292, acc: 0.607422]  [A loss: 0.697169, acc: 0.511719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "304: [D loss: 0.641818, acc: 0.611328]  [A loss: 1.044830, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "305: [D loss: 0.646762, acc: 0.619141]  [A loss: 0.753797, acc: 0.464844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "306: [D loss: 0.629066, acc: 0.615234]  [A loss: 0.964654, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "307: [D loss: 0.627190, acc: 0.646484]  [A loss: 0.843427, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "308: [D loss: 0.610616, acc: 0.658203]  [A loss: 0.950653, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "309: [D loss: 0.632722, acc: 0.625000]  [A loss: 0.826826, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "310: [D loss: 0.635276, acc: 0.644531]  [A loss: 0.964555, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "311: [D loss: 0.620796, acc: 0.660156]  [A loss: 0.892437, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "312: [D loss: 0.624731, acc: 0.656250]  [A loss: 0.907626, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "313: [D loss: 0.616059, acc: 0.662109]  [A loss: 0.906800, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "314: [D loss: 0.636124, acc: 0.626953]  [A loss: 0.965509, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "315: [D loss: 0.622256, acc: 0.683594]  [A loss: 0.835464, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "316: [D loss: 0.627288, acc: 0.669922]  [A loss: 0.981332, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "317: [D loss: 0.646652, acc: 0.617188]  [A loss: 0.774345, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "318: [D loss: 0.618826, acc: 0.638672]  [A loss: 1.188589, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "319: [D loss: 0.639155, acc: 0.636719]  [A loss: 0.603185, acc: 0.718750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "320: [D loss: 0.685013, acc: 0.574219]  [A loss: 1.322409, acc: 0.023438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "321: [D loss: 0.694366, acc: 0.564453]  [A loss: 0.717077, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "322: [D loss: 0.666643, acc: 0.593750]  [A loss: 0.976354, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "323: [D loss: 0.601701, acc: 0.693359]  [A loss: 0.817894, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "324: [D loss: 0.609082, acc: 0.677734]  [A loss: 0.973492, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "325: [D loss: 0.619191, acc: 0.666016]  [A loss: 0.824840, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "326: [D loss: 0.590236, acc: 0.683594]  [A loss: 0.937827, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "327: [D loss: 0.625608, acc: 0.654297]  [A loss: 0.881954, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "328: [D loss: 0.602552, acc: 0.687500]  [A loss: 0.912316, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "329: [D loss: 0.620112, acc: 0.646484]  [A loss: 0.996221, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "330: [D loss: 0.619246, acc: 0.652344]  [A loss: 0.809359, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "331: [D loss: 0.625001, acc: 0.642578]  [A loss: 0.944916, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "332: [D loss: 0.629213, acc: 0.630859]  [A loss: 0.858419, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "333: [D loss: 0.639293, acc: 0.642578]  [A loss: 0.947447, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "334: [D loss: 0.630681, acc: 0.628906]  [A loss: 0.836796, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "335: [D loss: 0.635042, acc: 0.626953]  [A loss: 1.100770, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "336: [D loss: 0.637190, acc: 0.640625]  [A loss: 0.698466, acc: 0.515625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "337: [D loss: 0.624168, acc: 0.652344]  [A loss: 1.248219, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "338: [D loss: 0.651056, acc: 0.603516]  [A loss: 0.664681, acc: 0.562500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "339: [D loss: 0.694711, acc: 0.548828]  [A loss: 1.169631, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "340: [D loss: 0.661114, acc: 0.605469]  [A loss: 0.740800, acc: 0.445312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "341: [D loss: 0.628127, acc: 0.632812]  [A loss: 1.006475, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "342: [D loss: 0.615607, acc: 0.673828]  [A loss: 0.801870, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "343: [D loss: 0.624691, acc: 0.658203]  [A loss: 0.962640, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "344: [D loss: 0.641682, acc: 0.625000]  [A loss: 0.774275, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "345: [D loss: 0.621856, acc: 0.669922]  [A loss: 0.899303, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "346: [D loss: 0.614487, acc: 0.654297]  [A loss: 0.871065, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "347: [D loss: 0.635150, acc: 0.613281]  [A loss: 0.934204, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "348: [D loss: 0.608488, acc: 0.693359]  [A loss: 0.900675, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "349: [D loss: 0.608641, acc: 0.683594]  [A loss: 1.011448, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "350: [D loss: 0.641022, acc: 0.611328]  [A loss: 0.819664, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "351: [D loss: 0.613183, acc: 0.664062]  [A loss: 0.993868, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "352: [D loss: 0.637020, acc: 0.625000]  [A loss: 0.891872, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "353: [D loss: 0.616512, acc: 0.667969]  [A loss: 0.878551, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "354: [D loss: 0.636857, acc: 0.648438]  [A loss: 0.996100, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "355: [D loss: 0.629548, acc: 0.630859]  [A loss: 0.870898, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "356: [D loss: 0.605893, acc: 0.681641]  [A loss: 1.028503, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "357: [D loss: 0.608875, acc: 0.660156]  [A loss: 0.775126, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "358: [D loss: 0.643502, acc: 0.642578]  [A loss: 1.261086, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "359: [D loss: 0.674485, acc: 0.578125]  [A loss: 0.643071, acc: 0.617188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "360: [D loss: 0.651962, acc: 0.607422]  [A loss: 1.169653, acc: 0.042969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "361: [D loss: 0.638898, acc: 0.615234]  [A loss: 0.693503, acc: 0.523438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "362: [D loss: 0.631933, acc: 0.595703]  [A loss: 1.076001, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "363: [D loss: 0.627739, acc: 0.642578]  [A loss: 0.816028, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "364: [D loss: 0.628170, acc: 0.611328]  [A loss: 0.997092, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "365: [D loss: 0.630679, acc: 0.634766]  [A loss: 0.811067, acc: 0.378906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "366: [D loss: 0.620873, acc: 0.658203]  [A loss: 1.002471, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "367: [D loss: 0.620659, acc: 0.640625]  [A loss: 0.860184, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "368: [D loss: 0.623026, acc: 0.654297]  [A loss: 0.952736, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "369: [D loss: 0.643090, acc: 0.638672]  [A loss: 0.800327, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "370: [D loss: 0.605205, acc: 0.671875]  [A loss: 1.106195, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "371: [D loss: 0.629789, acc: 0.636719]  [A loss: 0.790200, acc: 0.421875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "372: [D loss: 0.637738, acc: 0.605469]  [A loss: 1.175717, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "373: [D loss: 0.644180, acc: 0.642578]  [A loss: 0.749838, acc: 0.417969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "374: [D loss: 0.650024, acc: 0.583984]  [A loss: 1.166465, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "375: [D loss: 0.642335, acc: 0.632812]  [A loss: 0.760539, acc: 0.460938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "376: [D loss: 0.623011, acc: 0.648438]  [A loss: 1.046570, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "377: [D loss: 0.625854, acc: 0.642578]  [A loss: 0.804438, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "378: [D loss: 0.610816, acc: 0.664062]  [A loss: 1.019879, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "379: [D loss: 0.623536, acc: 0.656250]  [A loss: 0.847767, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "380: [D loss: 0.629601, acc: 0.626953]  [A loss: 1.026065, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "381: [D loss: 0.629096, acc: 0.640625]  [A loss: 0.824535, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "382: [D loss: 0.614174, acc: 0.619141]  [A loss: 0.959160, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "383: [D loss: 0.614383, acc: 0.673828]  [A loss: 0.834123, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "384: [D loss: 0.619993, acc: 0.662109]  [A loss: 0.909730, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "385: [D loss: 0.606190, acc: 0.669922]  [A loss: 0.984956, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "386: [D loss: 0.624811, acc: 0.650391]  [A loss: 0.934963, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "387: [D loss: 0.607890, acc: 0.703125]  [A loss: 0.925527, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "388: [D loss: 0.620863, acc: 0.654297]  [A loss: 1.004091, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "389: [D loss: 0.591560, acc: 0.667969]  [A loss: 0.876290, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "390: [D loss: 0.630043, acc: 0.636719]  [A loss: 1.141721, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "391: [D loss: 0.628894, acc: 0.634766]  [A loss: 0.686748, acc: 0.542969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "392: [D loss: 0.652398, acc: 0.609375]  [A loss: 1.305476, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "393: [D loss: 0.680108, acc: 0.605469]  [A loss: 0.681543, acc: 0.558594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "394: [D loss: 0.661658, acc: 0.585938]  [A loss: 1.129907, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "395: [D loss: 0.624084, acc: 0.632812]  [A loss: 0.798332, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "396: [D loss: 0.612109, acc: 0.660156]  [A loss: 0.965922, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "397: [D loss: 0.619321, acc: 0.648438]  [A loss: 0.870022, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "398: [D loss: 0.595611, acc: 0.673828]  [A loss: 1.017245, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "399: [D loss: 0.647938, acc: 0.623047]  [A loss: 0.862911, acc: 0.312500]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "400: [D loss: 0.594961, acc: 0.689453]  [A loss: 1.009275, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "401: [D loss: 0.629384, acc: 0.634766]  [A loss: 0.888845, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "402: [D loss: 0.611213, acc: 0.681641]  [A loss: 0.859952, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "403: [D loss: 0.612534, acc: 0.667969]  [A loss: 1.041269, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "404: [D loss: 0.624281, acc: 0.644531]  [A loss: 0.843781, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "405: [D loss: 0.613817, acc: 0.666016]  [A loss: 1.063129, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "406: [D loss: 0.627278, acc: 0.652344]  [A loss: 0.786853, acc: 0.410156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "407: [D loss: 0.654149, acc: 0.613281]  [A loss: 1.306515, acc: 0.046875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "408: [D loss: 0.654240, acc: 0.601562]  [A loss: 0.629635, acc: 0.625000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "409: [D loss: 0.640938, acc: 0.628906]  [A loss: 1.230868, acc: 0.035156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "410: [D loss: 0.666365, acc: 0.585938]  [A loss: 0.762585, acc: 0.410156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "411: [D loss: 0.609461, acc: 0.650391]  [A loss: 1.007981, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "412: [D loss: 0.588027, acc: 0.699219]  [A loss: 0.906464, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "413: [D loss: 0.617103, acc: 0.656250]  [A loss: 0.974089, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "414: [D loss: 0.622178, acc: 0.667969]  [A loss: 0.888176, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "415: [D loss: 0.611795, acc: 0.667969]  [A loss: 0.948476, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "416: [D loss: 0.601713, acc: 0.679688]  [A loss: 0.891682, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "417: [D loss: 0.615211, acc: 0.662109]  [A loss: 0.995476, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "418: [D loss: 0.593486, acc: 0.687500]  [A loss: 0.870616, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "419: [D loss: 0.621085, acc: 0.656250]  [A loss: 1.134269, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "420: [D loss: 0.635302, acc: 0.615234]  [A loss: 0.756576, acc: 0.445312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "421: [D loss: 0.638633, acc: 0.644531]  [A loss: 1.134747, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "422: [D loss: 0.647924, acc: 0.597656]  [A loss: 0.746773, acc: 0.496094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "423: [D loss: 0.630349, acc: 0.656250]  [A loss: 1.101810, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "424: [D loss: 0.637579, acc: 0.642578]  [A loss: 0.784934, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "425: [D loss: 0.608170, acc: 0.677734]  [A loss: 1.067910, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "426: [D loss: 0.638305, acc: 0.613281]  [A loss: 0.857158, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "427: [D loss: 0.645765, acc: 0.619141]  [A loss: 0.974707, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "428: [D loss: 0.627167, acc: 0.646484]  [A loss: 0.819437, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "429: [D loss: 0.615855, acc: 0.646484]  [A loss: 1.038469, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "430: [D loss: 0.612328, acc: 0.697266]  [A loss: 0.848106, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "431: [D loss: 0.611181, acc: 0.658203]  [A loss: 1.038355, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "432: [D loss: 0.629078, acc: 0.660156]  [A loss: 0.775318, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "433: [D loss: 0.615069, acc: 0.648438]  [A loss: 1.146330, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "434: [D loss: 0.648955, acc: 0.625000]  [A loss: 0.745517, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "435: [D loss: 0.644212, acc: 0.601562]  [A loss: 1.117615, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "436: [D loss: 0.620290, acc: 0.644531]  [A loss: 0.823418, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "437: [D loss: 0.609603, acc: 0.667969]  [A loss: 1.095151, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "438: [D loss: 0.637217, acc: 0.652344]  [A loss: 0.806690, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "439: [D loss: 0.571245, acc: 0.724609]  [A loss: 1.032015, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "440: [D loss: 0.640622, acc: 0.632812]  [A loss: 0.892925, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "441: [D loss: 0.619382, acc: 0.638672]  [A loss: 0.948951, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "442: [D loss: 0.608145, acc: 0.662109]  [A loss: 0.933081, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "443: [D loss: 0.624836, acc: 0.636719]  [A loss: 1.040191, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "444: [D loss: 0.603161, acc: 0.691406]  [A loss: 0.834245, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "445: [D loss: 0.619532, acc: 0.669922]  [A loss: 1.220847, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "446: [D loss: 0.646660, acc: 0.617188]  [A loss: 0.762428, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "447: [D loss: 0.637428, acc: 0.632812]  [A loss: 1.212171, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "448: [D loss: 0.656746, acc: 0.634766]  [A loss: 0.773191, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "449: [D loss: 0.620469, acc: 0.660156]  [A loss: 1.049714, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "450: [D loss: 0.643882, acc: 0.626953]  [A loss: 0.842144, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "451: [D loss: 0.624992, acc: 0.636719]  [A loss: 0.959890, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "452: [D loss: 0.610373, acc: 0.671875]  [A loss: 0.935006, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "453: [D loss: 0.610181, acc: 0.671875]  [A loss: 0.944220, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "454: [D loss: 0.596169, acc: 0.669922]  [A loss: 0.981146, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "455: [D loss: 0.611287, acc: 0.685547]  [A loss: 0.906307, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "456: [D loss: 0.607121, acc: 0.691406]  [A loss: 0.973156, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "457: [D loss: 0.597380, acc: 0.695312]  [A loss: 0.860943, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "458: [D loss: 0.601160, acc: 0.656250]  [A loss: 1.110988, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "459: [D loss: 0.622759, acc: 0.664062]  [A loss: 0.795236, acc: 0.398438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "460: [D loss: 0.638568, acc: 0.646484]  [A loss: 1.146452, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "461: [D loss: 0.620579, acc: 0.650391]  [A loss: 0.706020, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "462: [D loss: 0.642364, acc: 0.611328]  [A loss: 1.282277, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "463: [D loss: 0.636241, acc: 0.625000]  [A loss: 0.754839, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "464: [D loss: 0.649140, acc: 0.630859]  [A loss: 1.187712, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "465: [D loss: 0.629108, acc: 0.638672]  [A loss: 0.814863, acc: 0.402344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "466: [D loss: 0.604164, acc: 0.664062]  [A loss: 1.011867, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "467: [D loss: 0.614853, acc: 0.662109]  [A loss: 0.944342, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "468: [D loss: 0.619312, acc: 0.669922]  [A loss: 0.984981, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "469: [D loss: 0.625388, acc: 0.683594]  [A loss: 0.945776, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "470: [D loss: 0.615698, acc: 0.664062]  [A loss: 0.900688, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "471: [D loss: 0.633719, acc: 0.638672]  [A loss: 0.919898, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "472: [D loss: 0.611097, acc: 0.660156]  [A loss: 1.041158, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "473: [D loss: 0.606564, acc: 0.679688]  [A loss: 0.803861, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "474: [D loss: 0.592765, acc: 0.673828]  [A loss: 1.131067, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "475: [D loss: 0.614941, acc: 0.673828]  [A loss: 0.770824, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "476: [D loss: 0.641562, acc: 0.630859]  [A loss: 1.184909, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "477: [D loss: 0.640988, acc: 0.636719]  [A loss: 0.763785, acc: 0.460938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "478: [D loss: 0.618627, acc: 0.667969]  [A loss: 1.094057, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "479: [D loss: 0.603394, acc: 0.666016]  [A loss: 0.829317, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "480: [D loss: 0.609892, acc: 0.683594]  [A loss: 1.024689, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "481: [D loss: 0.618284, acc: 0.652344]  [A loss: 0.910361, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "482: [D loss: 0.612494, acc: 0.687500]  [A loss: 0.929529, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "483: [D loss: 0.597470, acc: 0.693359]  [A loss: 1.051103, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "484: [D loss: 0.614697, acc: 0.664062]  [A loss: 0.922994, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "485: [D loss: 0.630668, acc: 0.667969]  [A loss: 0.962638, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "486: [D loss: 0.621917, acc: 0.650391]  [A loss: 0.989192, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "487: [D loss: 0.614831, acc: 0.656250]  [A loss: 0.833836, acc: 0.378906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "488: [D loss: 0.624474, acc: 0.632812]  [A loss: 1.107992, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "489: [D loss: 0.593138, acc: 0.679688]  [A loss: 0.887881, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "490: [D loss: 0.601674, acc: 0.671875]  [A loss: 1.206318, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "491: [D loss: 0.601876, acc: 0.689453]  [A loss: 0.707902, acc: 0.503906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "492: [D loss: 0.643375, acc: 0.603516]  [A loss: 1.283661, acc: 0.054688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "493: [D loss: 0.618527, acc: 0.644531]  [A loss: 0.745027, acc: 0.468750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "494: [D loss: 0.617878, acc: 0.630859]  [A loss: 1.160759, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "495: [D loss: 0.622173, acc: 0.654297]  [A loss: 0.832329, acc: 0.343750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "496: [D loss: 0.605756, acc: 0.669922]  [A loss: 1.042485, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "497: [D loss: 0.593516, acc: 0.710938]  [A loss: 0.872049, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "498: [D loss: 0.606045, acc: 0.691406]  [A loss: 1.042635, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "499: [D loss: 0.617541, acc: 0.650391]  [A loss: 0.865568, acc: 0.273438]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "500: [D loss: 0.601009, acc: 0.673828]  [A loss: 1.035271, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "501: [D loss: 0.616958, acc: 0.656250]  [A loss: 0.874544, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "502: [D loss: 0.609144, acc: 0.642578]  [A loss: 1.109811, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "503: [D loss: 0.593578, acc: 0.683594]  [A loss: 0.867473, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "504: [D loss: 0.596018, acc: 0.681641]  [A loss: 1.117580, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "505: [D loss: 0.623851, acc: 0.652344]  [A loss: 0.854261, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "506: [D loss: 0.618039, acc: 0.644531]  [A loss: 1.205154, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "507: [D loss: 0.634420, acc: 0.642578]  [A loss: 0.753023, acc: 0.484375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "508: [D loss: 0.651613, acc: 0.632812]  [A loss: 1.142497, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "509: [D loss: 0.631606, acc: 0.634766]  [A loss: 0.753989, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "510: [D loss: 0.617031, acc: 0.640625]  [A loss: 1.080731, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "511: [D loss: 0.621528, acc: 0.630859]  [A loss: 0.795535, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "512: [D loss: 0.610978, acc: 0.701172]  [A loss: 1.059976, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "513: [D loss: 0.581297, acc: 0.695312]  [A loss: 0.893597, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "514: [D loss: 0.605415, acc: 0.667969]  [A loss: 1.055908, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "515: [D loss: 0.614729, acc: 0.644531]  [A loss: 0.946942, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "516: [D loss: 0.628692, acc: 0.634766]  [A loss: 1.046964, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "517: [D loss: 0.611346, acc: 0.644531]  [A loss: 0.932125, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "518: [D loss: 0.605899, acc: 0.683594]  [A loss: 1.039479, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "519: [D loss: 0.601005, acc: 0.660156]  [A loss: 0.896716, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "520: [D loss: 0.612785, acc: 0.675781]  [A loss: 1.002648, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "521: [D loss: 0.604573, acc: 0.675781]  [A loss: 0.952817, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "522: [D loss: 0.599555, acc: 0.673828]  [A loss: 1.042097, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "523: [D loss: 0.620252, acc: 0.671875]  [A loss: 0.859522, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "524: [D loss: 0.619117, acc: 0.642578]  [A loss: 1.193123, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "525: [D loss: 0.629313, acc: 0.650391]  [A loss: 0.685291, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "526: [D loss: 0.639805, acc: 0.611328]  [A loss: 1.393195, acc: 0.015625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "527: [D loss: 0.655769, acc: 0.599609]  [A loss: 0.760301, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "528: [D loss: 0.599254, acc: 0.669922]  [A loss: 1.045903, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "529: [D loss: 0.608910, acc: 0.673828]  [A loss: 0.908964, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "530: [D loss: 0.604265, acc: 0.673828]  [A loss: 1.034830, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "531: [D loss: 0.599470, acc: 0.691406]  [A loss: 0.892352, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "532: [D loss: 0.620868, acc: 0.650391]  [A loss: 0.961463, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "533: [D loss: 0.577617, acc: 0.693359]  [A loss: 1.030795, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "534: [D loss: 0.593189, acc: 0.667969]  [A loss: 0.871441, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "535: [D loss: 0.591256, acc: 0.695312]  [A loss: 1.245858, acc: 0.058594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "536: [D loss: 0.620935, acc: 0.634766]  [A loss: 0.828912, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "537: [D loss: 0.613231, acc: 0.658203]  [A loss: 1.124502, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "538: [D loss: 0.601797, acc: 0.695312]  [A loss: 0.835349, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "539: [D loss: 0.572000, acc: 0.689453]  [A loss: 1.129561, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "540: [D loss: 0.607790, acc: 0.667969]  [A loss: 0.832837, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "541: [D loss: 0.602714, acc: 0.662109]  [A loss: 1.089202, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "542: [D loss: 0.626060, acc: 0.658203]  [A loss: 0.853146, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "543: [D loss: 0.612111, acc: 0.658203]  [A loss: 1.174805, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "544: [D loss: 0.609878, acc: 0.656250]  [A loss: 0.869889, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "545: [D loss: 0.588482, acc: 0.689453]  [A loss: 1.195972, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "546: [D loss: 0.608901, acc: 0.664062]  [A loss: 0.836660, acc: 0.382812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "547: [D loss: 0.625659, acc: 0.660156]  [A loss: 1.164330, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "548: [D loss: 0.609583, acc: 0.675781]  [A loss: 0.816660, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "549: [D loss: 0.619381, acc: 0.634766]  [A loss: 1.105861, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "550: [D loss: 0.598748, acc: 0.691406]  [A loss: 0.855204, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "551: [D loss: 0.600860, acc: 0.660156]  [A loss: 1.091467, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "552: [D loss: 0.602185, acc: 0.669922]  [A loss: 0.939988, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "553: [D loss: 0.601250, acc: 0.675781]  [A loss: 1.107125, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "554: [D loss: 0.602424, acc: 0.677734]  [A loss: 0.901582, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "555: [D loss: 0.607791, acc: 0.667969]  [A loss: 1.280235, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "556: [D loss: 0.652619, acc: 0.621094]  [A loss: 0.750216, acc: 0.457031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "557: [D loss: 0.634030, acc: 0.658203]  [A loss: 1.255410, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "558: [D loss: 0.608098, acc: 0.666016]  [A loss: 0.841739, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "559: [D loss: 0.601198, acc: 0.679688]  [A loss: 1.117345, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "560: [D loss: 0.599272, acc: 0.693359]  [A loss: 0.951055, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "561: [D loss: 0.577364, acc: 0.714844]  [A loss: 1.061723, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "562: [D loss: 0.613701, acc: 0.677734]  [A loss: 0.858266, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "563: [D loss: 0.589268, acc: 0.656250]  [A loss: 1.164072, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "564: [D loss: 0.599758, acc: 0.677734]  [A loss: 0.886571, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "565: [D loss: 0.597450, acc: 0.689453]  [A loss: 1.219697, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "566: [D loss: 0.612157, acc: 0.685547]  [A loss: 0.770779, acc: 0.441406]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "567: [D loss: 0.659234, acc: 0.626953]  [A loss: 1.163801, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "568: [D loss: 0.598557, acc: 0.675781]  [A loss: 0.753028, acc: 0.500000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "569: [D loss: 0.599104, acc: 0.679688]  [A loss: 1.127757, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "570: [D loss: 0.590298, acc: 0.685547]  [A loss: 0.905980, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "571: [D loss: 0.625668, acc: 0.638672]  [A loss: 1.196491, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "572: [D loss: 0.587059, acc: 0.707031]  [A loss: 0.866387, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "573: [D loss: 0.600139, acc: 0.689453]  [A loss: 1.144220, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "574: [D loss: 0.595798, acc: 0.658203]  [A loss: 0.844669, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "575: [D loss: 0.610724, acc: 0.660156]  [A loss: 1.203057, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "576: [D loss: 0.625754, acc: 0.654297]  [A loss: 0.872754, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "577: [D loss: 0.620401, acc: 0.662109]  [A loss: 1.060468, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "578: [D loss: 0.611937, acc: 0.656250]  [A loss: 0.928040, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "579: [D loss: 0.625511, acc: 0.646484]  [A loss: 1.047644, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "580: [D loss: 0.595669, acc: 0.683594]  [A loss: 0.895437, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "581: [D loss: 0.586463, acc: 0.673828]  [A loss: 1.224696, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "582: [D loss: 0.613392, acc: 0.646484]  [A loss: 0.986700, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "583: [D loss: 0.598064, acc: 0.666016]  [A loss: 1.149058, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "584: [D loss: 0.635275, acc: 0.625000]  [A loss: 0.773198, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "585: [D loss: 0.603348, acc: 0.666016]  [A loss: 1.031709, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "586: [D loss: 0.613956, acc: 0.677734]  [A loss: 0.922968, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "587: [D loss: 0.581892, acc: 0.671875]  [A loss: 1.149502, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "588: [D loss: 0.578190, acc: 0.697266]  [A loss: 0.992733, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "589: [D loss: 0.607428, acc: 0.666016]  [A loss: 1.044008, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "590: [D loss: 0.602281, acc: 0.669922]  [A loss: 0.868455, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "591: [D loss: 0.605563, acc: 0.660156]  [A loss: 1.216970, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "592: [D loss: 0.626907, acc: 0.638672]  [A loss: 0.815384, acc: 0.417969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "593: [D loss: 0.595933, acc: 0.662109]  [A loss: 1.193910, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "594: [D loss: 0.586597, acc: 0.673828]  [A loss: 0.850218, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "595: [D loss: 0.611167, acc: 0.677734]  [A loss: 1.139654, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "596: [D loss: 0.635416, acc: 0.638672]  [A loss: 0.932964, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "597: [D loss: 0.598926, acc: 0.660156]  [A loss: 1.089426, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "598: [D loss: 0.598281, acc: 0.669922]  [A loss: 0.879511, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "599: [D loss: 0.609695, acc: 0.679688]  [A loss: 1.141516, acc: 0.160156]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "600: [D loss: 0.630097, acc: 0.658203]  [A loss: 0.842033, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "601: [D loss: 0.611484, acc: 0.660156]  [A loss: 1.163521, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "602: [D loss: 0.594604, acc: 0.695312]  [A loss: 0.869808, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "603: [D loss: 0.632649, acc: 0.642578]  [A loss: 1.200259, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "604: [D loss: 0.619413, acc: 0.656250]  [A loss: 0.788138, acc: 0.429688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "605: [D loss: 0.610564, acc: 0.699219]  [A loss: 1.141939, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "606: [D loss: 0.626225, acc: 0.658203]  [A loss: 0.846939, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "607: [D loss: 0.601030, acc: 0.703125]  [A loss: 1.064273, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "608: [D loss: 0.600154, acc: 0.669922]  [A loss: 0.903765, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "609: [D loss: 0.610171, acc: 0.648438]  [A loss: 1.144896, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "610: [D loss: 0.607754, acc: 0.669922]  [A loss: 0.878747, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "611: [D loss: 0.604353, acc: 0.656250]  [A loss: 1.131367, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "612: [D loss: 0.614579, acc: 0.664062]  [A loss: 0.867012, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "613: [D loss: 0.601803, acc: 0.675781]  [A loss: 1.104896, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "614: [D loss: 0.596612, acc: 0.685547]  [A loss: 0.951080, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "615: [D loss: 0.593063, acc: 0.683594]  [A loss: 1.016522, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "616: [D loss: 0.589927, acc: 0.697266]  [A loss: 0.981913, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "617: [D loss: 0.593366, acc: 0.687500]  [A loss: 1.054200, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "618: [D loss: 0.577822, acc: 0.722656]  [A loss: 0.925224, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "619: [D loss: 0.600184, acc: 0.687500]  [A loss: 1.051773, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "620: [D loss: 0.584117, acc: 0.685547]  [A loss: 0.920626, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "621: [D loss: 0.598856, acc: 0.666016]  [A loss: 1.243868, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "622: [D loss: 0.633538, acc: 0.636719]  [A loss: 0.788961, acc: 0.417969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "623: [D loss: 0.565406, acc: 0.708984]  [A loss: 1.143201, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "624: [D loss: 0.602944, acc: 0.675781]  [A loss: 0.955706, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "625: [D loss: 0.668721, acc: 0.591797]  [A loss: 1.327921, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "626: [D loss: 0.618030, acc: 0.646484]  [A loss: 0.771667, acc: 0.453125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "627: [D loss: 0.645764, acc: 0.619141]  [A loss: 1.168701, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "628: [D loss: 0.597990, acc: 0.671875]  [A loss: 0.879052, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "629: [D loss: 0.641820, acc: 0.623047]  [A loss: 1.093651, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "630: [D loss: 0.615892, acc: 0.664062]  [A loss: 0.948820, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "631: [D loss: 0.627363, acc: 0.658203]  [A loss: 1.077248, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "632: [D loss: 0.586684, acc: 0.691406]  [A loss: 0.931087, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "633: [D loss: 0.608011, acc: 0.652344]  [A loss: 1.033963, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "634: [D loss: 0.602447, acc: 0.664062]  [A loss: 0.959242, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "635: [D loss: 0.582075, acc: 0.693359]  [A loss: 1.012509, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "636: [D loss: 0.597815, acc: 0.664062]  [A loss: 0.988773, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "637: [D loss: 0.606231, acc: 0.664062]  [A loss: 1.122312, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "638: [D loss: 0.600165, acc: 0.673828]  [A loss: 0.920774, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "639: [D loss: 0.613373, acc: 0.660156]  [A loss: 0.999568, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "640: [D loss: 0.575799, acc: 0.689453]  [A loss: 0.994622, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "641: [D loss: 0.567863, acc: 0.693359]  [A loss: 1.002815, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "642: [D loss: 0.624260, acc: 0.634766]  [A loss: 1.074053, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "643: [D loss: 0.604833, acc: 0.681641]  [A loss: 1.019050, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "644: [D loss: 0.611079, acc: 0.662109]  [A loss: 0.993701, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "645: [D loss: 0.627975, acc: 0.625000]  [A loss: 1.058299, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "646: [D loss: 0.595525, acc: 0.693359]  [A loss: 1.090807, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "647: [D loss: 0.593255, acc: 0.667969]  [A loss: 0.916867, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "648: [D loss: 0.636099, acc: 0.650391]  [A loss: 1.201894, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "649: [D loss: 0.574661, acc: 0.705078]  [A loss: 0.847046, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "650: [D loss: 0.636262, acc: 0.615234]  [A loss: 1.339193, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "651: [D loss: 0.617553, acc: 0.646484]  [A loss: 0.765355, acc: 0.464844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "652: [D loss: 0.616617, acc: 0.652344]  [A loss: 1.208002, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "653: [D loss: 0.616019, acc: 0.671875]  [A loss: 0.857106, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "654: [D loss: 0.581466, acc: 0.705078]  [A loss: 1.052516, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "655: [D loss: 0.592029, acc: 0.679688]  [A loss: 1.019168, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "656: [D loss: 0.582320, acc: 0.693359]  [A loss: 1.044066, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "657: [D loss: 0.571041, acc: 0.705078]  [A loss: 1.045511, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "658: [D loss: 0.598618, acc: 0.677734]  [A loss: 1.003799, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "659: [D loss: 0.581091, acc: 0.695312]  [A loss: 1.013706, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "660: [D loss: 0.590506, acc: 0.703125]  [A loss: 1.053088, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "661: [D loss: 0.560934, acc: 0.716797]  [A loss: 1.057350, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "662: [D loss: 0.595796, acc: 0.669922]  [A loss: 1.036118, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "663: [D loss: 0.590647, acc: 0.697266]  [A loss: 1.021576, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "664: [D loss: 0.592988, acc: 0.666016]  [A loss: 1.176727, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "665: [D loss: 0.598778, acc: 0.671875]  [A loss: 0.948529, acc: 0.304688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "666: [D loss: 0.580208, acc: 0.712891]  [A loss: 1.306966, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "667: [D loss: 0.593745, acc: 0.691406]  [A loss: 0.746628, acc: 0.437500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "668: [D loss: 0.616255, acc: 0.658203]  [A loss: 1.363399, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "669: [D loss: 0.609142, acc: 0.658203]  [A loss: 0.892805, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "670: [D loss: 0.644966, acc: 0.650391]  [A loss: 1.263321, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "671: [D loss: 0.637315, acc: 0.666016]  [A loss: 0.782164, acc: 0.472656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "672: [D loss: 0.610445, acc: 0.656250]  [A loss: 1.150937, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "673: [D loss: 0.577435, acc: 0.687500]  [A loss: 0.955260, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "674: [D loss: 0.645880, acc: 0.636719]  [A loss: 1.195485, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "675: [D loss: 0.619786, acc: 0.632812]  [A loss: 0.845521, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "676: [D loss: 0.612123, acc: 0.660156]  [A loss: 1.124184, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "677: [D loss: 0.592983, acc: 0.681641]  [A loss: 1.065847, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "678: [D loss: 0.595460, acc: 0.683594]  [A loss: 1.051994, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "679: [D loss: 0.564918, acc: 0.701172]  [A loss: 1.047223, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "680: [D loss: 0.567100, acc: 0.740234]  [A loss: 0.986824, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "681: [D loss: 0.571195, acc: 0.707031]  [A loss: 1.089660, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "682: [D loss: 0.601917, acc: 0.675781]  [A loss: 1.037587, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "683: [D loss: 0.605032, acc: 0.687500]  [A loss: 1.025122, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "684: [D loss: 0.616428, acc: 0.632812]  [A loss: 1.137244, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "685: [D loss: 0.591344, acc: 0.650391]  [A loss: 0.922573, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "686: [D loss: 0.610700, acc: 0.648438]  [A loss: 1.169757, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "687: [D loss: 0.593693, acc: 0.712891]  [A loss: 0.868797, acc: 0.363281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "688: [D loss: 0.585887, acc: 0.691406]  [A loss: 1.236526, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "689: [D loss: 0.598336, acc: 0.683594]  [A loss: 0.863911, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "690: [D loss: 0.636213, acc: 0.646484]  [A loss: 1.289461, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "691: [D loss: 0.612715, acc: 0.642578]  [A loss: 0.831895, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "692: [D loss: 0.596792, acc: 0.667969]  [A loss: 1.136621, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "693: [D loss: 0.609476, acc: 0.691406]  [A loss: 1.033968, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "694: [D loss: 0.577502, acc: 0.712891]  [A loss: 1.031006, acc: 0.242188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "695: [D loss: 0.589950, acc: 0.693359]  [A loss: 0.981828, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "696: [D loss: 0.578473, acc: 0.716797]  [A loss: 1.053604, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "697: [D loss: 0.615433, acc: 0.664062]  [A loss: 1.095816, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "698: [D loss: 0.615691, acc: 0.664062]  [A loss: 1.039540, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "699: [D loss: 0.623143, acc: 0.654297]  [A loss: 1.067062, acc: 0.164062]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "700: [D loss: 0.590891, acc: 0.669922]  [A loss: 0.962794, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "701: [D loss: 0.582596, acc: 0.695312]  [A loss: 1.155962, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "702: [D loss: 0.585151, acc: 0.707031]  [A loss: 0.902281, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "703: [D loss: 0.633623, acc: 0.662109]  [A loss: 1.288296, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "704: [D loss: 0.636905, acc: 0.644531]  [A loss: 0.783438, acc: 0.417969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "705: [D loss: 0.619422, acc: 0.638672]  [A loss: 1.206458, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "706: [D loss: 0.602872, acc: 0.681641]  [A loss: 0.888705, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "707: [D loss: 0.623971, acc: 0.650391]  [A loss: 1.217874, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "708: [D loss: 0.579650, acc: 0.691406]  [A loss: 0.882259, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "709: [D loss: 0.596183, acc: 0.666016]  [A loss: 1.185557, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "710: [D loss: 0.627841, acc: 0.626953]  [A loss: 0.917419, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "711: [D loss: 0.607432, acc: 0.693359]  [A loss: 1.073280, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "712: [D loss: 0.598576, acc: 0.675781]  [A loss: 0.999585, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "713: [D loss: 0.578470, acc: 0.703125]  [A loss: 1.061445, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "714: [D loss: 0.579512, acc: 0.705078]  [A loss: 1.112726, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "715: [D loss: 0.595287, acc: 0.675781]  [A loss: 1.027964, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "716: [D loss: 0.623859, acc: 0.656250]  [A loss: 1.086326, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "717: [D loss: 0.593404, acc: 0.687500]  [A loss: 0.908287, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "718: [D loss: 0.588070, acc: 0.664062]  [A loss: 1.151069, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "719: [D loss: 0.617107, acc: 0.666016]  [A loss: 0.810911, acc: 0.375000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "720: [D loss: 0.610989, acc: 0.689453]  [A loss: 1.274139, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "721: [D loss: 0.642059, acc: 0.638672]  [A loss: 0.758692, acc: 0.464844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "722: [D loss: 0.594064, acc: 0.695312]  [A loss: 1.202385, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "723: [D loss: 0.605424, acc: 0.683594]  [A loss: 0.962099, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "724: [D loss: 0.635018, acc: 0.660156]  [A loss: 1.138833, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "725: [D loss: 0.590440, acc: 0.712891]  [A loss: 0.943938, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "726: [D loss: 0.574122, acc: 0.714844]  [A loss: 1.128200, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "727: [D loss: 0.613153, acc: 0.669922]  [A loss: 0.920030, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "728: [D loss: 0.582222, acc: 0.708984]  [A loss: 1.181731, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "729: [D loss: 0.620778, acc: 0.640625]  [A loss: 0.893099, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "730: [D loss: 0.596157, acc: 0.697266]  [A loss: 1.155663, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "731: [D loss: 0.586629, acc: 0.666016]  [A loss: 0.923299, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "732: [D loss: 0.590655, acc: 0.652344]  [A loss: 1.139190, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "733: [D loss: 0.628238, acc: 0.640625]  [A loss: 1.035117, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "734: [D loss: 0.613026, acc: 0.650391]  [A loss: 0.971730, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "735: [D loss: 0.603303, acc: 0.675781]  [A loss: 1.049698, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "736: [D loss: 0.569232, acc: 0.716797]  [A loss: 1.038322, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "737: [D loss: 0.585318, acc: 0.693359]  [A loss: 1.003633, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "738: [D loss: 0.615732, acc: 0.642578]  [A loss: 1.202124, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "739: [D loss: 0.595099, acc: 0.675781]  [A loss: 0.812233, acc: 0.414062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "740: [D loss: 0.606443, acc: 0.660156]  [A loss: 1.336307, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "741: [D loss: 0.611623, acc: 0.652344]  [A loss: 0.864444, acc: 0.347656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "742: [D loss: 0.602348, acc: 0.679688]  [A loss: 1.241122, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "743: [D loss: 0.635434, acc: 0.617188]  [A loss: 0.810839, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "744: [D loss: 0.617204, acc: 0.654297]  [A loss: 1.230852, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "745: [D loss: 0.603611, acc: 0.644531]  [A loss: 0.944363, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "746: [D loss: 0.602660, acc: 0.687500]  [A loss: 1.073870, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "747: [D loss: 0.573418, acc: 0.691406]  [A loss: 0.924032, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "748: [D loss: 0.606973, acc: 0.648438]  [A loss: 1.073475, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "749: [D loss: 0.585405, acc: 0.691406]  [A loss: 0.927046, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "750: [D loss: 0.601119, acc: 0.681641]  [A loss: 1.151017, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "751: [D loss: 0.600561, acc: 0.685547]  [A loss: 0.907826, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "752: [D loss: 0.590741, acc: 0.693359]  [A loss: 1.194089, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "753: [D loss: 0.574523, acc: 0.697266]  [A loss: 1.021893, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "754: [D loss: 0.628750, acc: 0.662109]  [A loss: 1.202527, acc: 0.117188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "755: [D loss: 0.586044, acc: 0.677734]  [A loss: 0.808973, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "756: [D loss: 0.614396, acc: 0.656250]  [A loss: 1.270271, acc: 0.105469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "757: [D loss: 0.592418, acc: 0.687500]  [A loss: 0.864154, acc: 0.414062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "758: [D loss: 0.602322, acc: 0.652344]  [A loss: 1.125414, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "759: [D loss: 0.593823, acc: 0.654297]  [A loss: 1.041012, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "760: [D loss: 0.609791, acc: 0.673828]  [A loss: 1.025287, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "761: [D loss: 0.576473, acc: 0.703125]  [A loss: 1.145230, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "762: [D loss: 0.605995, acc: 0.675781]  [A loss: 0.981164, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "763: [D loss: 0.580802, acc: 0.699219]  [A loss: 1.083192, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "764: [D loss: 0.590484, acc: 0.675781]  [A loss: 1.146874, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "765: [D loss: 0.621754, acc: 0.666016]  [A loss: 0.992922, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "766: [D loss: 0.598397, acc: 0.679688]  [A loss: 1.014080, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "767: [D loss: 0.591472, acc: 0.689453]  [A loss: 0.940854, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "768: [D loss: 0.586866, acc: 0.679688]  [A loss: 1.069897, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "769: [D loss: 0.615756, acc: 0.654297]  [A loss: 1.009471, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "770: [D loss: 0.576842, acc: 0.693359]  [A loss: 1.006548, acc: 0.265625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "771: [D loss: 0.591561, acc: 0.691406]  [A loss: 1.396449, acc: 0.050781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "772: [D loss: 0.645362, acc: 0.630859]  [A loss: 0.795777, acc: 0.410156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "773: [D loss: 0.628519, acc: 0.638672]  [A loss: 1.282922, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "774: [D loss: 0.591408, acc: 0.687500]  [A loss: 0.854232, acc: 0.390625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "775: [D loss: 0.590584, acc: 0.673828]  [A loss: 1.131007, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "776: [D loss: 0.614446, acc: 0.662109]  [A loss: 1.080939, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "777: [D loss: 0.576446, acc: 0.697266]  [A loss: 0.915957, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "778: [D loss: 0.574423, acc: 0.693359]  [A loss: 1.218887, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "779: [D loss: 0.572215, acc: 0.708984]  [A loss: 0.932343, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "780: [D loss: 0.624094, acc: 0.646484]  [A loss: 1.270436, acc: 0.093750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "781: [D loss: 0.625804, acc: 0.640625]  [A loss: 0.877831, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "782: [D loss: 0.628203, acc: 0.648438]  [A loss: 1.070821, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "783: [D loss: 0.595039, acc: 0.701172]  [A loss: 0.954797, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "784: [D loss: 0.586023, acc: 0.673828]  [A loss: 1.032353, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "785: [D loss: 0.598476, acc: 0.675781]  [A loss: 0.942897, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "786: [D loss: 0.602961, acc: 0.652344]  [A loss: 1.096194, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "787: [D loss: 0.595644, acc: 0.687500]  [A loss: 0.904999, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "788: [D loss: 0.589036, acc: 0.708984]  [A loss: 1.206297, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "789: [D loss: 0.568351, acc: 0.703125]  [A loss: 0.868634, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "790: [D loss: 0.636391, acc: 0.636719]  [A loss: 1.209263, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "791: [D loss: 0.625590, acc: 0.654297]  [A loss: 0.822927, acc: 0.351562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "792: [D loss: 0.595012, acc: 0.693359]  [A loss: 1.178757, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "793: [D loss: 0.605513, acc: 0.673828]  [A loss: 0.950855, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "794: [D loss: 0.583511, acc: 0.699219]  [A loss: 1.108338, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "795: [D loss: 0.583875, acc: 0.683594]  [A loss: 0.966390, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "796: [D loss: 0.602393, acc: 0.683594]  [A loss: 1.165146, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "797: [D loss: 0.611111, acc: 0.656250]  [A loss: 0.893120, acc: 0.363281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "798: [D loss: 0.599797, acc: 0.695312]  [A loss: 1.045001, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "799: [D loss: 0.613028, acc: 0.656250]  [A loss: 1.162690, acc: 0.148438]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "800: [D loss: 0.614860, acc: 0.662109]  [A loss: 0.920869, acc: 0.277344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "801: [D loss: 0.612978, acc: 0.669922]  [A loss: 1.144354, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "802: [D loss: 0.598215, acc: 0.671875]  [A loss: 0.893887, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "803: [D loss: 0.585328, acc: 0.726562]  [A loss: 1.098712, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "804: [D loss: 0.573346, acc: 0.707031]  [A loss: 0.991110, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "805: [D loss: 0.639699, acc: 0.626953]  [A loss: 1.082168, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "806: [D loss: 0.612281, acc: 0.677734]  [A loss: 0.915228, acc: 0.339844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "807: [D loss: 0.583283, acc: 0.697266]  [A loss: 1.129506, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "808: [D loss: 0.575468, acc: 0.697266]  [A loss: 0.917379, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "809: [D loss: 0.599372, acc: 0.671875]  [A loss: 1.147102, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "810: [D loss: 0.572780, acc: 0.712891]  [A loss: 0.900921, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "811: [D loss: 0.609847, acc: 0.677734]  [A loss: 1.360786, acc: 0.066406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "812: [D loss: 0.628834, acc: 0.642578]  [A loss: 0.766587, acc: 0.425781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "813: [D loss: 0.617627, acc: 0.664062]  [A loss: 1.311592, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "814: [D loss: 0.621398, acc: 0.646484]  [A loss: 0.887924, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "815: [D loss: 0.586085, acc: 0.681641]  [A loss: 1.056359, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "816: [D loss: 0.607689, acc: 0.664062]  [A loss: 1.104074, acc: 0.191406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "817: [D loss: 0.623925, acc: 0.630859]  [A loss: 1.104916, acc: 0.136719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "818: [D loss: 0.616413, acc: 0.671875]  [A loss: 1.019391, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "819: [D loss: 0.580839, acc: 0.716797]  [A loss: 0.987705, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "820: [D loss: 0.587767, acc: 0.703125]  [A loss: 1.091988, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "821: [D loss: 0.580162, acc: 0.712891]  [A loss: 0.990726, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "822: [D loss: 0.592660, acc: 0.687500]  [A loss: 1.148170, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "823: [D loss: 0.605804, acc: 0.669922]  [A loss: 1.017901, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "824: [D loss: 0.607688, acc: 0.673828]  [A loss: 1.035696, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "825: [D loss: 0.593838, acc: 0.671875]  [A loss: 1.098572, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "826: [D loss: 0.593377, acc: 0.705078]  [A loss: 1.043337, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "827: [D loss: 0.585112, acc: 0.699219]  [A loss: 1.078713, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "828: [D loss: 0.599291, acc: 0.675781]  [A loss: 1.016034, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "829: [D loss: 0.581306, acc: 0.671875]  [A loss: 1.119819, acc: 0.175781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "830: [D loss: 0.588818, acc: 0.673828]  [A loss: 1.013439, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "831: [D loss: 0.601182, acc: 0.677734]  [A loss: 1.151299, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "832: [D loss: 0.610359, acc: 0.666016]  [A loss: 0.925483, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "833: [D loss: 0.582148, acc: 0.701172]  [A loss: 1.180520, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "834: [D loss: 0.597403, acc: 0.671875]  [A loss: 0.967612, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "835: [D loss: 0.583920, acc: 0.697266]  [A loss: 1.172234, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "836: [D loss: 0.591500, acc: 0.683594]  [A loss: 0.922740, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "837: [D loss: 0.636321, acc: 0.640625]  [A loss: 1.292800, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "838: [D loss: 0.626288, acc: 0.625000]  [A loss: 0.727150, acc: 0.523438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "839: [D loss: 0.642448, acc: 0.626953]  [A loss: 1.252165, acc: 0.070312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "840: [D loss: 0.595901, acc: 0.683594]  [A loss: 0.908440, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "841: [D loss: 0.606594, acc: 0.666016]  [A loss: 1.157506, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "842: [D loss: 0.610717, acc: 0.666016]  [A loss: 0.907702, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "843: [D loss: 0.608354, acc: 0.669922]  [A loss: 1.137307, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "844: [D loss: 0.582258, acc: 0.699219]  [A loss: 0.894663, acc: 0.328125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "845: [D loss: 0.581887, acc: 0.697266]  [A loss: 1.143334, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "846: [D loss: 0.596969, acc: 0.697266]  [A loss: 0.922587, acc: 0.332031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "847: [D loss: 0.622650, acc: 0.662109]  [A loss: 1.165088, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "848: [D loss: 0.601331, acc: 0.669922]  [A loss: 0.902385, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "849: [D loss: 0.595531, acc: 0.666016]  [A loss: 1.204499, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "850: [D loss: 0.606994, acc: 0.623047]  [A loss: 0.868196, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "851: [D loss: 0.598619, acc: 0.687500]  [A loss: 1.080588, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "852: [D loss: 0.597775, acc: 0.667969]  [A loss: 0.984413, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "853: [D loss: 0.563642, acc: 0.732422]  [A loss: 1.002595, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "854: [D loss: 0.634013, acc: 0.666016]  [A loss: 1.126310, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "855: [D loss: 0.599883, acc: 0.681641]  [A loss: 0.898513, acc: 0.343750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "856: [D loss: 0.616016, acc: 0.687500]  [A loss: 1.151618, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "857: [D loss: 0.616069, acc: 0.662109]  [A loss: 1.012660, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "858: [D loss: 0.571655, acc: 0.687500]  [A loss: 1.162730, acc: 0.121094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "859: [D loss: 0.575394, acc: 0.689453]  [A loss: 0.914134, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "860: [D loss: 0.609487, acc: 0.660156]  [A loss: 1.151138, acc: 0.128906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "861: [D loss: 0.558729, acc: 0.716797]  [A loss: 1.006080, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "862: [D loss: 0.626982, acc: 0.630859]  [A loss: 1.078305, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "863: [D loss: 0.600512, acc: 0.689453]  [A loss: 0.934874, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "864: [D loss: 0.600464, acc: 0.654297]  [A loss: 1.060312, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "865: [D loss: 0.599804, acc: 0.689453]  [A loss: 0.992195, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "866: [D loss: 0.617010, acc: 0.677734]  [A loss: 1.111269, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "867: [D loss: 0.578774, acc: 0.699219]  [A loss: 1.035679, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "868: [D loss: 0.606848, acc: 0.691406]  [A loss: 1.176673, acc: 0.109375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "869: [D loss: 0.602849, acc: 0.675781]  [A loss: 0.924489, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "870: [D loss: 0.600236, acc: 0.701172]  [A loss: 1.305607, acc: 0.082031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "871: [D loss: 0.620338, acc: 0.650391]  [A loss: 0.805275, acc: 0.449219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "872: [D loss: 0.614494, acc: 0.664062]  [A loss: 1.275325, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "873: [D loss: 0.603680, acc: 0.669922]  [A loss: 0.947153, acc: 0.246094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "874: [D loss: 0.592510, acc: 0.671875]  [A loss: 1.091753, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "875: [D loss: 0.600740, acc: 0.695312]  [A loss: 1.000923, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "876: [D loss: 0.574424, acc: 0.691406]  [A loss: 1.146260, acc: 0.140625]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "877: [D loss: 0.608500, acc: 0.664062]  [A loss: 0.951665, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "878: [D loss: 0.617736, acc: 0.666016]  [A loss: 1.107409, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "879: [D loss: 0.593354, acc: 0.667969]  [A loss: 1.020883, acc: 0.257812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "880: [D loss: 0.584101, acc: 0.685547]  [A loss: 1.063886, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "881: [D loss: 0.590835, acc: 0.699219]  [A loss: 1.023308, acc: 0.210938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "882: [D loss: 0.583011, acc: 0.685547]  [A loss: 1.143422, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "883: [D loss: 0.599688, acc: 0.673828]  [A loss: 0.932536, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "884: [D loss: 0.598310, acc: 0.660156]  [A loss: 1.233702, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "885: [D loss: 0.594815, acc: 0.677734]  [A loss: 0.884534, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "886: [D loss: 0.627687, acc: 0.650391]  [A loss: 1.345986, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "887: [D loss: 0.642745, acc: 0.638672]  [A loss: 0.749994, acc: 0.472656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "888: [D loss: 0.614245, acc: 0.656250]  [A loss: 1.193277, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "889: [D loss: 0.578326, acc: 0.697266]  [A loss: 0.992253, acc: 0.261719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "890: [D loss: 0.596654, acc: 0.675781]  [A loss: 1.200285, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "891: [D loss: 0.606833, acc: 0.683594]  [A loss: 0.950277, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "892: [D loss: 0.606357, acc: 0.697266]  [A loss: 1.091854, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "893: [D loss: 0.586695, acc: 0.685547]  [A loss: 0.988506, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "894: [D loss: 0.597971, acc: 0.675781]  [A loss: 1.063331, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "895: [D loss: 0.582398, acc: 0.687500]  [A loss: 0.990209, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "896: [D loss: 0.587498, acc: 0.701172]  [A loss: 1.090812, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "897: [D loss: 0.608078, acc: 0.679688]  [A loss: 1.066077, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "898: [D loss: 0.586370, acc: 0.699219]  [A loss: 1.021159, acc: 0.269531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "899: [D loss: 0.598756, acc: 0.699219]  [A loss: 1.141885, acc: 0.160156]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "900: [D loss: 0.596017, acc: 0.660156]  [A loss: 0.954260, acc: 0.289062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "901: [D loss: 0.592896, acc: 0.693359]  [A loss: 1.349804, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "902: [D loss: 0.661956, acc: 0.613281]  [A loss: 0.723283, acc: 0.531250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "903: [D loss: 0.612168, acc: 0.664062]  [A loss: 1.270076, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "904: [D loss: 0.617821, acc: 0.673828]  [A loss: 0.919311, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "905: [D loss: 0.584122, acc: 0.701172]  [A loss: 1.119654, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "906: [D loss: 0.594128, acc: 0.697266]  [A loss: 1.053365, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "907: [D loss: 0.606445, acc: 0.687500]  [A loss: 1.047081, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "908: [D loss: 0.591512, acc: 0.687500]  [A loss: 0.985098, acc: 0.226562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "909: [D loss: 0.576529, acc: 0.691406]  [A loss: 1.083002, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "910: [D loss: 0.590840, acc: 0.687500]  [A loss: 1.100325, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "911: [D loss: 0.618199, acc: 0.677734]  [A loss: 1.051465, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "912: [D loss: 0.583198, acc: 0.681641]  [A loss: 1.105270, acc: 0.167969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "913: [D loss: 0.601490, acc: 0.671875]  [A loss: 1.024234, acc: 0.238281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "914: [D loss: 0.601206, acc: 0.673828]  [A loss: 1.026636, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "915: [D loss: 0.619183, acc: 0.646484]  [A loss: 1.082142, acc: 0.203125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "916: [D loss: 0.579106, acc: 0.691406]  [A loss: 0.931386, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "917: [D loss: 0.599170, acc: 0.677734]  [A loss: 1.186105, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "918: [D loss: 0.614010, acc: 0.669922]  [A loss: 0.919384, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "919: [D loss: 0.636132, acc: 0.656250]  [A loss: 1.347382, acc: 0.078125]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "920: [D loss: 0.627813, acc: 0.650391]  [A loss: 0.813004, acc: 0.386719]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "921: [D loss: 0.588228, acc: 0.703125]  [A loss: 1.149334, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "922: [D loss: 0.597317, acc: 0.691406]  [A loss: 0.856796, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "923: [D loss: 0.617952, acc: 0.656250]  [A loss: 1.193610, acc: 0.089844]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "924: [D loss: 0.619329, acc: 0.652344]  [A loss: 0.857455, acc: 0.343750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "925: [D loss: 0.606691, acc: 0.683594]  [A loss: 1.159032, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "926: [D loss: 0.598593, acc: 0.677734]  [A loss: 0.898254, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "927: [D loss: 0.568300, acc: 0.730469]  [A loss: 1.107599, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "928: [D loss: 0.572821, acc: 0.699219]  [A loss: 1.075559, acc: 0.164062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "929: [D loss: 0.607724, acc: 0.677734]  [A loss: 1.068856, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "930: [D loss: 0.578634, acc: 0.707031]  [A loss: 0.928343, acc: 0.273438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "931: [D loss: 0.583195, acc: 0.714844]  [A loss: 1.245782, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "932: [D loss: 0.638930, acc: 0.654297]  [A loss: 0.854538, acc: 0.355469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "933: [D loss: 0.578569, acc: 0.691406]  [A loss: 1.225451, acc: 0.085938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "934: [D loss: 0.611033, acc: 0.664062]  [A loss: 0.907091, acc: 0.359375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "935: [D loss: 0.594142, acc: 0.685547]  [A loss: 1.138119, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "936: [D loss: 0.623858, acc: 0.656250]  [A loss: 0.872192, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "937: [D loss: 0.630122, acc: 0.646484]  [A loss: 1.171013, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "938: [D loss: 0.591952, acc: 0.660156]  [A loss: 0.899483, acc: 0.316406]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "939: [D loss: 0.613961, acc: 0.648438]  [A loss: 1.100630, acc: 0.187500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "940: [D loss: 0.609341, acc: 0.666016]  [A loss: 0.956543, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "941: [D loss: 0.608469, acc: 0.687500]  [A loss: 1.152375, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "942: [D loss: 0.595642, acc: 0.681641]  [A loss: 0.918142, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "943: [D loss: 0.626780, acc: 0.642578]  [A loss: 1.236547, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "944: [D loss: 0.619307, acc: 0.648438]  [A loss: 0.889339, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "945: [D loss: 0.614926, acc: 0.671875]  [A loss: 1.155740, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "946: [D loss: 0.587302, acc: 0.701172]  [A loss: 0.981891, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "947: [D loss: 0.617636, acc: 0.691406]  [A loss: 1.108462, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "948: [D loss: 0.591839, acc: 0.667969]  [A loss: 0.920621, acc: 0.308594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "949: [D loss: 0.620935, acc: 0.654297]  [A loss: 1.036729, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "950: [D loss: 0.590535, acc: 0.691406]  [A loss: 0.950579, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "951: [D loss: 0.597370, acc: 0.689453]  [A loss: 1.078069, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "952: [D loss: 0.628099, acc: 0.634766]  [A loss: 0.937544, acc: 0.300781]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "953: [D loss: 0.594767, acc: 0.667969]  [A loss: 0.992987, acc: 0.222656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "954: [D loss: 0.589497, acc: 0.689453]  [A loss: 1.186478, acc: 0.160156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "955: [D loss: 0.598383, acc: 0.673828]  [A loss: 0.983357, acc: 0.234375]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "956: [D loss: 0.629996, acc: 0.660156]  [A loss: 1.036894, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "957: [D loss: 0.600832, acc: 0.679688]  [A loss: 0.924864, acc: 0.335938]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "958: [D loss: 0.571417, acc: 0.689453]  [A loss: 1.175150, acc: 0.132812]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "959: [D loss: 0.597085, acc: 0.681641]  [A loss: 0.896406, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "960: [D loss: 0.588791, acc: 0.669922]  [A loss: 1.268197, acc: 0.101562]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "961: [D loss: 0.615762, acc: 0.654297]  [A loss: 0.932033, acc: 0.312500]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "962: [D loss: 0.600483, acc: 0.660156]  [A loss: 1.233939, acc: 0.097656]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "963: [D loss: 0.623278, acc: 0.683594]  [A loss: 0.828292, acc: 0.367188]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "964: [D loss: 0.607502, acc: 0.667969]  [A loss: 1.270973, acc: 0.074219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "965: [D loss: 0.623947, acc: 0.625000]  [A loss: 0.826658, acc: 0.406250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "966: [D loss: 0.612381, acc: 0.638672]  [A loss: 1.265261, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "967: [D loss: 0.620939, acc: 0.654297]  [A loss: 0.922083, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "968: [D loss: 0.611332, acc: 0.654297]  [A loss: 1.064331, acc: 0.171875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "969: [D loss: 0.560322, acc: 0.726562]  [A loss: 0.957325, acc: 0.285156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "970: [D loss: 0.602848, acc: 0.664062]  [A loss: 1.119825, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "971: [D loss: 0.583290, acc: 0.703125]  [A loss: 0.986692, acc: 0.281250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "972: [D loss: 0.576199, acc: 0.697266]  [A loss: 1.119958, acc: 0.156250]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "973: [D loss: 0.587807, acc: 0.695312]  [A loss: 0.997869, acc: 0.207031]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "974: [D loss: 0.620227, acc: 0.652344]  [A loss: 1.065064, acc: 0.199219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "975: [D loss: 0.619075, acc: 0.662109]  [A loss: 0.925556, acc: 0.296875]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "976: [D loss: 0.589481, acc: 0.697266]  [A loss: 1.232548, acc: 0.125000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "977: [D loss: 0.590163, acc: 0.691406]  [A loss: 0.987251, acc: 0.253906]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "978: [D loss: 0.584929, acc: 0.687500]  [A loss: 1.082362, acc: 0.152344]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "979: [D loss: 0.577513, acc: 0.695312]  [A loss: 1.065764, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "980: [D loss: 0.584968, acc: 0.695312]  [A loss: 1.180263, acc: 0.183594]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "981: [D loss: 0.618521, acc: 0.642578]  [A loss: 0.941548, acc: 0.292969]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "982: [D loss: 0.579194, acc: 0.708984]  [A loss: 1.124118, acc: 0.148438]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "983: [D loss: 0.592934, acc: 0.677734]  [A loss: 0.901524, acc: 0.324219]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "984: [D loss: 0.606653, acc: 0.669922]  [A loss: 1.187257, acc: 0.113281]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "985: [D loss: 0.596172, acc: 0.681641]  [A loss: 0.896891, acc: 0.320312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "986: [D loss: 0.677452, acc: 0.611328]  [A loss: 1.473701, acc: 0.039062]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "987: [D loss: 0.690587, acc: 0.595703]  [A loss: 0.793198, acc: 0.410156]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "988: [D loss: 0.627323, acc: 0.646484]  [A loss: 1.113104, acc: 0.144531]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "989: [D loss: 0.587234, acc: 0.677734]  [A loss: 1.020700, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "990: [D loss: 0.587551, acc: 0.695312]  [A loss: 1.066362, acc: 0.195312]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "991: [D loss: 0.614865, acc: 0.654297]  [A loss: 1.016639, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "992: [D loss: 0.566960, acc: 0.724609]  [A loss: 1.016410, acc: 0.214844]\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "993: [D loss: 0.585103, acc: 0.691406]  [A loss: 1.041782, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "994: [D loss: 0.616275, acc: 0.658203]  [A loss: 1.024902, acc: 0.230469]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "995: [D loss: 0.622294, acc: 0.662109]  [A loss: 1.107982, acc: 0.218750]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "996: [D loss: 0.594481, acc: 0.695312]  [A loss: 0.981534, acc: 0.250000]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "997: [D loss: 0.620121, acc: 0.660156]  [A loss: 1.129582, acc: 0.179688]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "998: [D loss: 0.607791, acc: 0.667969]  [A loss: 0.874471, acc: 0.371094]\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "999: [D loss: 0.610009, acc: 0.652344]  [A loss: 1.229830, acc: 0.105469]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Elapsed: 3.7023059606552122 min \n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timer = ElapsedTimer()\n",
    "train(train_epochs=1000, batch_size=256, save_interval=100) \n",
    "timer.elapsed_time()\n",
    "plot_images(fake=True)\n",
    "plot_images(fake=False, saveToFile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1. Explica qué hacen las siguientes líneas de código:**\n",
    "\n",
    "```python\n",
    "timer = ElapsedTimer()\n",
    "train(train_epochs=1000, batch_size=256, save_interval=100)\n",
    "timer.elapsed_time()\n",
    "plot_images(fake=True)\n",
    "plot_images(fake=False, saveToFile=True)\n",
    "```\n",
    "\n",
    "Este código mide el tiempo de entrenamiento mediante una instancia de la clase ElapsedTimer, entrena el modelo GAN, imprime el tiempo transcurrido y genera y muestra gráficos de imágenes reales y generadas. \n",
    "\n",
    "En mayor detalle:\n",
    "    - La linea timer = ElapsedTimer(): Crea una instancia de la clase ElapsedTimer, iniciando el temporizador y la medición del tiempo.\n",
    "    - La linea train(train_epochs=1000, batch_size=256, save_interval=100): Llama a la función train con los parámetros especificados. Inicia el proceso de entrenamiento de la red adversaria para un total de 1000 épocas, con un batch_size de 256 y guardando las imágenes generadas cada 100 épocas.\n",
    "    - La linea timer.elapsed_time(): Aquí se llama al método elapsed_time() de la instancia ElapsedTimer. Calcula e imprimer el tiempo transcurrido desde que se inició el temporizador. \n",
    "    - La linea plot_images(fake=True): Llama a la función plot_images() para generar y mostrar gráficos de las imágenes generadas. El parámetro fake=True indica que la función debe mostrar las imágenes generadas por el modelo GAN.\n",
    "    - La linea plot_images(fake=False, saveToFile=True):  Llama nuevamente a la función plot_images() para graficar imágenes reales y además indica que las imágenes deben guardarse en un archivo.\n",
    "   \n",
    "\n",
    "\n",
    "**2. Escribe el código necesario para mostrar las imágenes generadas en la última iteración y muestra los resultados:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:23:54.760895Z",
     "iopub.status.busy": "2023-06-08T12:23:54.760518Z",
     "iopub.status.idle": "2023-06-08T12:23:55.501740Z",
     "shell.execute_reply": "2023-06-08T12:23:55.500796Z",
     "shell.execute_reply.started": "2023-06-08T12:23:54.760847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(fake=True, epoch=1000) #Serían las imágenes generadas en la última epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T12:23:55.504091Z",
     "iopub.status.busy": "2023-06-08T12:23:55.503705Z",
     "iopub.status.idle": "2023-06-08T12:23:56.209671Z",
     "shell.execute_reply": "2023-06-08T12:23:56.208776Z",
     "shell.execute_reply.started": "2023-06-08T12:23:55.504055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(fake=False, epoch=1000) #Las imagenes reales correspondientes a esa epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias consultadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/keywords-to-know-before-you-start-reading-papers-on-gans-8a08a665b40c\n",
    "* https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/\n",
    "* Modelos Generativos - Jordi de la Torre Gallart\n",
    "* https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/\n",
    "* https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "* https://towardsdatascience.com/batch-normalisation-in-deep-neural-network-ce65dd9e8dbf\n",
    "* https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "* https://medium.com/swlh/gan-generative-adversarial-network-3706ebfef77e\n",
    "* https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "* https://www.techtarget.com/searchenterpriseai/feature/CNN-vs-GAN-How-are-they-different\n",
    "* Fundamentos de las redes neuronales convolucionales por Jordi Casas Roma y Anna Bosch Rue\n",
    "* https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a\n",
    "* https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be\n",
    "* https://keras.io/api/optimizers/rmsprop/\n",
    "* https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
